{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython.display as ipd\n",
    "\n",
    "for filename in os.listdir('./test_audio'):\n",
    "    ipd.Audio('./test_audio/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "for filename in os.listdir('./test'):\n",
    "    data, sampling_rate = librosa.load('./test/' + filename)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = sr.AudioFile('0_jackson_2.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zero as source:\n",
    "    audio = r.record(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.recognize_google(audio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the main training part:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training with audio files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports:\n",
    "\n",
    "from export_model import *\n",
    "from preprocess import *\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving vectors of label - '1': 100%|█████████████████████████████████████████████████| 312/312 [00:05<00:00, 57.43it/s]\n",
      "Saving vectors of label - '10': 100%|████████████████████████████████████████████████| 143/143 [00:02<00:00, 60.06it/s]\n",
      "Saving vectors of label - '11': 100%|████████████████████████████████████████████████| 137/137 [00:02<00:00, 58.37it/s]\n",
      "Saving vectors of label - '12': 100%|████████████████████████████████████████████████| 132/132 [00:02<00:00, 51.72it/s]\n",
      "Saving vectors of label - '2': 100%|█████████████████████████████████████████████████| 272/272 [00:04<00:00, 60.04it/s]\n",
      "Saving vectors of label - '3': 100%|█████████████████████████████████████████████████| 250/250 [00:03<00:00, 64.88it/s]\n",
      "Saving vectors of label - '4': 100%|█████████████████████████████████████████████████| 139/139 [00:02<00:00, 60.78it/s]\n",
      "Saving vectors of label - '5': 100%|█████████████████████████████████████████████████| 166/166 [00:02<00:00, 60.50it/s]\n",
      "Saving vectors of label - '6': 100%|█████████████████████████████████████████████████| 145/145 [00:02<00:00, 57.24it/s]\n",
      "Saving vectors of label - '7': 100%|█████████████████████████████████████████████████| 150/150 [00:02<00:00, 52.61it/s]\n",
      "Saving vectors of label - '8': 100%|█████████████████████████████████████████████████| 154/154 [00:02<00:00, 62.00it/s]\n",
      "Saving vectors of label - '9': 100%|█████████████████████████████████████████████████| 139/139 [00:02<00:00, 56.83it/s]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#from preprocess import *\n",
    "#import keras\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "#from keras.utils import to_categorical\n",
    "#from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#import itertools\n",
    "\n",
    "# Second dimension of the feature is dim2\n",
    "feature_dim_2 = 11\n",
    "\n",
    "# Save data to array file first\n",
    "save_data_to_array(max_len=feature_dim_2)\n",
    "\n",
    "# # Loading train set and test set\n",
    "X_train, X_test, y_train, y_test = get_train_test()\n",
    "\n",
    "# # Feature dimension\n",
    "#defaults at the end\n",
    "feature_dim_1 = 20   #20\n",
    "channel = 1          #1\n",
    "epochs = 50         #50\n",
    "batch_size = 100     #100\n",
    "verbose = 1          #1\n",
    "# change num_classes depending on the amount of labels\n",
    "num_classes = 12\n",
    "\n",
    "# Reshaping to perform 2D convolution\n",
    "X_train = X_train.reshape(X_train.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "X_test = X_test.reshape(X_test.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model & prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(feature_dim_1, feature_dim_2, channel)))\n",
    "    model.add(Conv2D(48, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(120, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Predicts one sample\n",
    "def predict(filepath, model):\n",
    "    sample = wav2mfcc(filepath)\n",
    "    sample_reshaped = sample.reshape(1, feature_dim_1, feature_dim_2, channel)\n",
    "    return get_labels()[0][\n",
    "            np.argmax(model.predict(sample_reshaped))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1283 samples, validate on 856 samples\n",
      "Epoch 1/50\n",
      "1283/1283 [==============================] - ETA: 11s - loss: 7.9039 - acc: 0.03 - ETA: 6s - loss: 6.3143 - acc: 0.0300 - ETA: 4s - loss: 5.3269 - acc: 0.060 - ETA: 3s - loss: 4.7524 - acc: 0.070 - ETA: 2s - loss: 4.3514 - acc: 0.080 - ETA: 2s - loss: 4.0329 - acc: 0.098 - ETA: 1s - loss: 3.8077 - acc: 0.104 - ETA: 1s - loss: 3.6459 - acc: 0.110 - ETA: 0s - loss: 3.4838 - acc: 0.131 - ETA: 0s - loss: 3.3659 - acc: 0.143 - ETA: 0s - loss: 3.2968 - acc: 0.141 - ETA: 0s - loss: 3.2222 - acc: 0.145 - 3s 3ms/step - loss: 3.1492 - acc: 0.1567 - val_loss: 1.9228 - val_acc: 0.4439\n",
      "Epoch 2/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 2.1278 - acc: 0.290 - ETA: 1s - loss: 2.1282 - acc: 0.260 - ETA: 1s - loss: 2.1642 - acc: 0.243 - ETA: 1s - loss: 2.1645 - acc: 0.245 - ETA: 1s - loss: 2.1262 - acc: 0.270 - ETA: 1s - loss: 2.0840 - acc: 0.285 - ETA: 0s - loss: 2.0578 - acc: 0.288 - ETA: 0s - loss: 2.0317 - acc: 0.301 - ETA: 0s - loss: 2.0379 - acc: 0.300 - ETA: 0s - loss: 2.0171 - acc: 0.312 - ETA: 0s - loss: 2.0051 - acc: 0.312 - ETA: 0s - loss: 1.9762 - acc: 0.324 - 2s 2ms/step - loss: 1.9543 - acc: 0.3320 - val_loss: 1.4325 - val_acc: 0.5900\n",
      "Epoch 3/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 1.7917 - acc: 0.370 - ETA: 1s - loss: 1.7079 - acc: 0.395 - ETA: 1s - loss: 1.6376 - acc: 0.433 - ETA: 1s - loss: 1.6065 - acc: 0.445 - ETA: 1s - loss: 1.5596 - acc: 0.454 - ETA: 1s - loss: 1.5134 - acc: 0.471 - ETA: 0s - loss: 1.4929 - acc: 0.481 - ETA: 0s - loss: 1.4740 - acc: 0.492 - ETA: 0s - loss: 1.4819 - acc: 0.490 - ETA: 0s - loss: 1.4657 - acc: 0.492 - ETA: 0s - loss: 1.4493 - acc: 0.496 - ETA: 0s - loss: 1.4419 - acc: 0.502 - 2s 2ms/step - loss: 1.4324 - acc: 0.5058 - val_loss: 0.6784 - val_acc: 0.8505\n",
      "Epoch 4/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 1.0943 - acc: 0.650 - ETA: 1s - loss: 1.1786 - acc: 0.590 - ETA: 1s - loss: 1.1255 - acc: 0.620 - ETA: 1s - loss: 1.0948 - acc: 0.627 - ETA: 1s - loss: 1.0954 - acc: 0.634 - ETA: 1s - loss: 1.0450 - acc: 0.653 - ETA: 0s - loss: 1.0456 - acc: 0.650 - ETA: 0s - loss: 1.0572 - acc: 0.641 - ETA: 0s - loss: 1.0683 - acc: 0.641 - ETA: 0s - loss: 1.0599 - acc: 0.646 - ETA: 0s - loss: 1.0514 - acc: 0.650 - ETA: 0s - loss: 1.0630 - acc: 0.645 - 2s 2ms/step - loss: 1.0462 - acc: 0.6508 - val_loss: 0.4605 - val_acc: 0.8843\n",
      "Epoch 5/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 1.0633 - acc: 0.670 - ETA: 1s - loss: 0.9624 - acc: 0.705 - ETA: 1s - loss: 0.9551 - acc: 0.713 - ETA: 1s - loss: 0.9571 - acc: 0.707 - ETA: 1s - loss: 0.9308 - acc: 0.706 - ETA: 1s - loss: 0.9427 - acc: 0.701 - ETA: 0s - loss: 0.9216 - acc: 0.708 - ETA: 0s - loss: 0.8950 - acc: 0.721 - ETA: 0s - loss: 0.8682 - acc: 0.728 - ETA: 0s - loss: 0.8707 - acc: 0.725 - ETA: 0s - loss: 0.8699 - acc: 0.727 - ETA: 0s - loss: 0.8696 - acc: 0.724 - 2s 2ms/step - loss: 0.8551 - acc: 0.7256 - val_loss: 0.3453 - val_acc: 0.8984\n",
      "Epoch 6/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.5967 - acc: 0.800 - ETA: 1s - loss: 0.6522 - acc: 0.775 - ETA: 1s - loss: 0.6353 - acc: 0.776 - ETA: 1s - loss: 0.5968 - acc: 0.785 - ETA: 1s - loss: 0.6020 - acc: 0.790 - ETA: 1s - loss: 0.6066 - acc: 0.788 - ETA: 0s - loss: 0.6109 - acc: 0.790 - ETA: 0s - loss: 0.6274 - acc: 0.790 - ETA: 0s - loss: 0.6293 - acc: 0.785 - ETA: 0s - loss: 0.6412 - acc: 0.780 - ETA: 0s - loss: 0.6363 - acc: 0.783 - ETA: 0s - loss: 0.6435 - acc: 0.780 - 3s 2ms/step - loss: 0.6348 - acc: 0.7825 - val_loss: 0.3178 - val_acc: 0.8925\n",
      "Epoch 7/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.5305 - acc: 0.830 - ETA: 1s - loss: 0.5791 - acc: 0.815 - ETA: 1s - loss: 0.5453 - acc: 0.826 - ETA: 1s - loss: 0.4843 - acc: 0.847 - ETA: 1s - loss: 0.5301 - acc: 0.830 - ETA: 1s - loss: 0.5582 - acc: 0.825 - ETA: 1s - loss: 0.5625 - acc: 0.815 - ETA: 0s - loss: 0.5596 - acc: 0.816 - ETA: 0s - loss: 0.5652 - acc: 0.814 - ETA: 0s - loss: 0.5804 - acc: 0.806 - ETA: 0s - loss: 0.5866 - acc: 0.805 - ETA: 0s - loss: 0.5738 - acc: 0.808 - 3s 2ms/step - loss: 0.5714 - acc: 0.8114 - val_loss: 0.2814 - val_acc: 0.8984\n",
      "Epoch 8/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.3949 - acc: 0.890 - ETA: 1s - loss: 0.4777 - acc: 0.830 - ETA: 1s - loss: 0.4699 - acc: 0.833 - ETA: 1s - loss: 0.5251 - acc: 0.812 - ETA: 1s - loss: 0.5368 - acc: 0.816 - ETA: 1s - loss: 0.5260 - acc: 0.823 - ETA: 1s - loss: 0.5420 - acc: 0.815 - ETA: 0s - loss: 0.5449 - acc: 0.816 - ETA: 0s - loss: 0.5283 - acc: 0.823 - ETA: 0s - loss: 0.5178 - acc: 0.826 - ETA: 0s - loss: 0.5017 - acc: 0.830 - ETA: 0s - loss: 0.4877 - acc: 0.837 - 3s 2ms/step - loss: 0.4807 - acc: 0.8402 - val_loss: 0.2717 - val_acc: 0.8960\n",
      "Epoch 9/50\n",
      "1283/1283 [==============================] - ETA: 2s - loss: 0.4095 - acc: 0.850 - ETA: 1s - loss: 0.4886 - acc: 0.835 - ETA: 1s - loss: 0.4422 - acc: 0.846 - ETA: 1s - loss: 0.4414 - acc: 0.850 - ETA: 1s - loss: 0.4662 - acc: 0.848 - ETA: 1s - loss: 0.4484 - acc: 0.853 - ETA: 0s - loss: 0.4416 - acc: 0.855 - ETA: 0s - loss: 0.4412 - acc: 0.853 - ETA: 0s - loss: 0.4543 - acc: 0.847 - ETA: 0s - loss: 0.4463 - acc: 0.851 - ETA: 0s - loss: 0.4412 - acc: 0.851 - ETA: 0s - loss: 0.4293 - acc: 0.855 - 2s 2ms/step - loss: 0.4277 - acc: 0.8566 - val_loss: 0.2534 - val_acc: 0.9019\n",
      "Epoch 10/50\n",
      "1283/1283 [==============================] - ETA: 2s - loss: 0.1517 - acc: 0.960 - ETA: 1s - loss: 0.4150 - acc: 0.895 - ETA: 1s - loss: 0.5090 - acc: 0.853 - ETA: 1s - loss: 0.4461 - acc: 0.867 - ETA: 1s - loss: 0.4201 - acc: 0.870 - ETA: 1s - loss: 0.4392 - acc: 0.863 - ETA: 1s - loss: 0.4363 - acc: 0.861 - ETA: 0s - loss: 0.4337 - acc: 0.861 - ETA: 0s - loss: 0.4228 - acc: 0.866 - ETA: 0s - loss: 0.4528 - acc: 0.859 - ETA: 0s - loss: 0.4586 - acc: 0.853 - ETA: 0s - loss: 0.4478 - acc: 0.858 - 3s 2ms/step - loss: 0.4495 - acc: 0.8566 - val_loss: 0.2498 - val_acc: 0.9089\n",
      "Epoch 11/50\n",
      "1283/1283 [==============================] - ETA: 2s - loss: 0.3114 - acc: 0.930 - ETA: 1s - loss: 0.3538 - acc: 0.880 - ETA: 1s - loss: 0.3527 - acc: 0.870 - ETA: 1s - loss: 0.3894 - acc: 0.860 - ETA: 1s - loss: 0.3908 - acc: 0.860 - ETA: 1s - loss: 0.4034 - acc: 0.856 - ETA: 1s - loss: 0.3679 - acc: 0.870 - ETA: 0s - loss: 0.3515 - acc: 0.875 - ETA: 0s - loss: 0.3412 - acc: 0.880 - ETA: 0s - loss: 0.3477 - acc: 0.878 - ETA: 0s - loss: 0.3523 - acc: 0.876 - ETA: 0s - loss: 0.3549 - acc: 0.875 - 3s 2ms/step - loss: 0.3577 - acc: 0.8737 - val_loss: 0.2643 - val_acc: 0.9007\n",
      "Epoch 12/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.2843 - acc: 0.880 - ETA: 1s - loss: 0.3163 - acc: 0.900 - ETA: 1s - loss: 0.3023 - acc: 0.900 - ETA: 1s - loss: 0.3273 - acc: 0.892 - ETA: 1s - loss: 0.3241 - acc: 0.888 - ETA: 1s - loss: 0.3204 - acc: 0.891 - ETA: 0s - loss: 0.3305 - acc: 0.885 - ETA: 0s - loss: 0.3319 - acc: 0.886 - ETA: 0s - loss: 0.3273 - acc: 0.890 - ETA: 0s - loss: 0.3193 - acc: 0.892 - ETA: 0s - loss: 0.3120 - acc: 0.894 - ETA: 0s - loss: 0.3020 - acc: 0.896 - 3s 2ms/step - loss: 0.3177 - acc: 0.8909 - val_loss: 0.2539 - val_acc: 0.9065\n",
      "Epoch 13/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.3241 - acc: 0.900 - ETA: 1s - loss: 0.2426 - acc: 0.925 - ETA: 1s - loss: 0.2452 - acc: 0.926 - ETA: 1s - loss: 0.2645 - acc: 0.922 - ETA: 1s - loss: 0.2950 - acc: 0.910 - ETA: 1s - loss: 0.2948 - acc: 0.906 - ETA: 1s - loss: 0.3033 - acc: 0.905 - ETA: 0s - loss: 0.3016 - acc: 0.903 - ETA: 0s - loss: 0.2951 - acc: 0.906 - ETA: 0s - loss: 0.3019 - acc: 0.898 - ETA: 0s - loss: 0.3089 - acc: 0.895 - ETA: 0s - loss: 0.3066 - acc: 0.895 - 3s 2ms/step - loss: 0.2968 - acc: 0.8995 - val_loss: 0.2415 - val_acc: 0.8984\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1283/1283 [==============================] - ETA: 1s - loss: 0.2934 - acc: 0.880 - ETA: 1s - loss: 0.3623 - acc: 0.875 - ETA: 1s - loss: 0.3374 - acc: 0.873 - ETA: 1s - loss: 0.3177 - acc: 0.885 - ETA: 1s - loss: 0.3395 - acc: 0.868 - ETA: 1s - loss: 0.3535 - acc: 0.863 - ETA: 1s - loss: 0.3509 - acc: 0.865 - ETA: 0s - loss: 0.3481 - acc: 0.868 - ETA: 0s - loss: 0.3474 - acc: 0.868 - ETA: 0s - loss: 0.3330 - acc: 0.877 - ETA: 0s - loss: 0.3284 - acc: 0.880 - ETA: 0s - loss: 0.3128 - acc: 0.885 - 3s 2ms/step - loss: 0.3042 - acc: 0.8878 - val_loss: 0.2567 - val_acc: 0.8995\n",
      "Epoch 15/50\n",
      "1283/1283 [==============================] - ETA: 2s - loss: 0.2968 - acc: 0.910 - ETA: 1s - loss: 0.3002 - acc: 0.905 - ETA: 1s - loss: 0.2729 - acc: 0.916 - ETA: 1s - loss: 0.2639 - acc: 0.910 - ETA: 1s - loss: 0.2590 - acc: 0.904 - ETA: 1s - loss: 0.2687 - acc: 0.903 - ETA: 1s - loss: 0.2688 - acc: 0.902 - ETA: 0s - loss: 0.2712 - acc: 0.902 - ETA: 0s - loss: 0.2705 - acc: 0.904 - ETA: 0s - loss: 0.2633 - acc: 0.908 - ETA: 0s - loss: 0.2647 - acc: 0.904 - ETA: 0s - loss: 0.2607 - acc: 0.905 - 3s 2ms/step - loss: 0.2654 - acc: 0.9057 - val_loss: 0.2310 - val_acc: 0.9147\n",
      "Epoch 16/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.1169 - acc: 0.960 - ETA: 1s - loss: 0.1928 - acc: 0.920 - ETA: 1s - loss: 0.2393 - acc: 0.916 - ETA: 1s - loss: 0.2189 - acc: 0.930 - ETA: 1s - loss: 0.2222 - acc: 0.928 - ETA: 1s - loss: 0.2221 - acc: 0.928 - ETA: 0s - loss: 0.2282 - acc: 0.924 - ETA: 0s - loss: 0.2442 - acc: 0.913 - ETA: 0s - loss: 0.2515 - acc: 0.907 - ETA: 0s - loss: 0.2588 - acc: 0.905 - ETA: 0s - loss: 0.2570 - acc: 0.907 - ETA: 0s - loss: 0.2629 - acc: 0.906 - 2s 2ms/step - loss: 0.2659 - acc: 0.9065 - val_loss: 0.2417 - val_acc: 0.9136\n",
      "Epoch 17/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.3072 - acc: 0.900 - ETA: 1s - loss: 0.2819 - acc: 0.905 - ETA: 1s - loss: 0.2813 - acc: 0.913 - ETA: 1s - loss: 0.2830 - acc: 0.907 - ETA: 1s - loss: 0.2870 - acc: 0.908 - ETA: 1s - loss: 0.2870 - acc: 0.905 - ETA: 0s - loss: 0.2849 - acc: 0.902 - ETA: 0s - loss: 0.2696 - acc: 0.907 - ETA: 0s - loss: 0.2706 - acc: 0.907 - ETA: 0s - loss: 0.2581 - acc: 0.914 - ETA: 0s - loss: 0.2583 - acc: 0.910 - ETA: 0s - loss: 0.2505 - acc: 0.910 - 2s 2ms/step - loss: 0.2519 - acc: 0.9104 - val_loss: 0.2936 - val_acc: 0.8995\n",
      "Epoch 18/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.3660 - acc: 0.900 - ETA: 1s - loss: 0.3132 - acc: 0.895 - ETA: 1s - loss: 0.2774 - acc: 0.913 - ETA: 1s - loss: 0.2859 - acc: 0.907 - ETA: 1s - loss: 0.2926 - acc: 0.906 - ETA: 1s - loss: 0.2841 - acc: 0.906 - ETA: 0s - loss: 0.2836 - acc: 0.900 - ETA: 0s - loss: 0.2717 - acc: 0.906 - ETA: 0s - loss: 0.2646 - acc: 0.910 - ETA: 0s - loss: 0.2685 - acc: 0.909 - ETA: 0s - loss: 0.2582 - acc: 0.911 - ETA: 0s - loss: 0.2533 - acc: 0.911 - 2s 2ms/step - loss: 0.2521 - acc: 0.9119 - val_loss: 0.2376 - val_acc: 0.9147\n",
      "Epoch 19/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.2302 - acc: 0.930 - ETA: 1s - loss: 0.1932 - acc: 0.945 - ETA: 1s - loss: 0.2008 - acc: 0.926 - ETA: 1s - loss: 0.2286 - acc: 0.925 - ETA: 1s - loss: 0.2222 - acc: 0.918 - ETA: 1s - loss: 0.2123 - acc: 0.925 - ETA: 0s - loss: 0.2048 - acc: 0.928 - ETA: 0s - loss: 0.2104 - acc: 0.922 - ETA: 0s - loss: 0.2095 - acc: 0.923 - ETA: 0s - loss: 0.2075 - acc: 0.924 - ETA: 0s - loss: 0.2128 - acc: 0.923 - ETA: 0s - loss: 0.2076 - acc: 0.925 - 2s 2ms/step - loss: 0.2043 - acc: 0.9275 - val_loss: 0.2402 - val_acc: 0.9077\n",
      "Epoch 20/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.1137 - acc: 0.960 - ETA: 1s - loss: 0.1718 - acc: 0.935 - ETA: 1s - loss: 0.1782 - acc: 0.933 - ETA: 1s - loss: 0.1808 - acc: 0.937 - ETA: 1s - loss: 0.2084 - acc: 0.930 - ETA: 1s - loss: 0.1976 - acc: 0.933 - ETA: 0s - loss: 0.1901 - acc: 0.934 - ETA: 0s - loss: 0.1976 - acc: 0.933 - ETA: 0s - loss: 0.1982 - acc: 0.933 - ETA: 0s - loss: 0.1990 - acc: 0.930 - ETA: 0s - loss: 0.2057 - acc: 0.928 - ETA: 0s - loss: 0.2222 - acc: 0.924 - 2s 2ms/step - loss: 0.2244 - acc: 0.9236 - val_loss: 0.2280 - val_acc: 0.9159\n",
      "Epoch 21/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.1782 - acc: 0.940 - ETA: 1s - loss: 0.1531 - acc: 0.950 - ETA: 1s - loss: 0.1658 - acc: 0.943 - ETA: 1s - loss: 0.1900 - acc: 0.937 - ETA: 1s - loss: 0.1870 - acc: 0.936 - ETA: 1s - loss: 0.1835 - acc: 0.940 - ETA: 0s - loss: 0.1779 - acc: 0.942 - ETA: 0s - loss: 0.1667 - acc: 0.945 - ETA: 0s - loss: 0.1642 - acc: 0.945 - ETA: 0s - loss: 0.1856 - acc: 0.939 - ETA: 0s - loss: 0.1845 - acc: 0.939 - ETA: 0s - loss: 0.1868 - acc: 0.937 - 2s 2ms/step - loss: 0.1985 - acc: 0.9353 - val_loss: 0.2418 - val_acc: 0.9100\n",
      "Epoch 22/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.1438 - acc: 0.950 - ETA: 1s - loss: 0.1393 - acc: 0.950 - ETA: 1s - loss: 0.1410 - acc: 0.946 - ETA: 1s - loss: 0.1740 - acc: 0.942 - ETA: 1s - loss: 0.1832 - acc: 0.936 - ETA: 1s - loss: 0.1764 - acc: 0.938 - ETA: 0s - loss: 0.1886 - acc: 0.934 - ETA: 0s - loss: 0.1981 - acc: 0.928 - ETA: 0s - loss: 0.1869 - acc: 0.933 - ETA: 0s - loss: 0.1874 - acc: 0.935 - ETA: 0s - loss: 0.1988 - acc: 0.932 - ETA: 0s - loss: 0.2041 - acc: 0.930 - 2s 2ms/step - loss: 0.2126 - acc: 0.9283 - val_loss: 0.2247 - val_acc: 0.9159\n",
      "Epoch 23/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.1258 - acc: 0.960 - ETA: 1s - loss: 0.1144 - acc: 0.960 - ETA: 1s - loss: 0.1161 - acc: 0.960 - ETA: 1s - loss: 0.1355 - acc: 0.952 - ETA: 1s - loss: 0.1424 - acc: 0.952 - ETA: 1s - loss: 0.1413 - acc: 0.955 - ETA: 0s - loss: 0.1424 - acc: 0.952 - ETA: 0s - loss: 0.1451 - acc: 0.951 - ETA: 0s - loss: 0.1480 - acc: 0.950 - ETA: 0s - loss: 0.1681 - acc: 0.943 - ETA: 0s - loss: 0.1768 - acc: 0.940 - ETA: 0s - loss: 0.1786 - acc: 0.938 - 2s 2ms/step - loss: 0.1865 - acc: 0.9361 - val_loss: 0.2536 - val_acc: 0.9100\n",
      "Epoch 24/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.1970 - acc: 0.920 - ETA: 1s - loss: 0.1939 - acc: 0.925 - ETA: 1s - loss: 0.1846 - acc: 0.926 - ETA: 1s - loss: 0.1683 - acc: 0.940 - ETA: 1s - loss: 0.1666 - acc: 0.942 - ETA: 1s - loss: 0.1717 - acc: 0.938 - ETA: 0s - loss: 0.1827 - acc: 0.928 - ETA: 0s - loss: 0.1884 - acc: 0.928 - ETA: 0s - loss: 0.1843 - acc: 0.930 - ETA: 0s - loss: 0.1902 - acc: 0.930 - ETA: 0s - loss: 0.1827 - acc: 0.932 - ETA: 0s - loss: 0.1953 - acc: 0.929 - 2s 2ms/step - loss: 0.1960 - acc: 0.9275 - val_loss: 0.2288 - val_acc: 0.9124\n",
      "Epoch 25/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.1888 - acc: 0.930 - ETA: 1s - loss: 0.2277 - acc: 0.920 - ETA: 1s - loss: 0.2041 - acc: 0.930 - ETA: 1s - loss: 0.1884 - acc: 0.937 - ETA: 1s - loss: 0.1905 - acc: 0.934 - ETA: 1s - loss: 0.1916 - acc: 0.930 - ETA: 0s - loss: 0.1876 - acc: 0.930 - ETA: 0s - loss: 0.1718 - acc: 0.936 - ETA: 0s - loss: 0.1629 - acc: 0.940 - ETA: 0s - loss: 0.1629 - acc: 0.943 - ETA: 0s - loss: 0.1659 - acc: 0.942 - ETA: 0s - loss: 0.1688 - acc: 0.940 - 2s 2ms/step - loss: 0.1634 - acc: 0.9423 - val_loss: 0.2342 - val_acc: 0.9077\n",
      "Epoch 26/50\n",
      "1283/1283 [==============================] - ETA: 2s - loss: 0.1773 - acc: 0.920 - ETA: 1s - loss: 0.1408 - acc: 0.935 - ETA: 1s - loss: 0.1567 - acc: 0.940 - ETA: 1s - loss: 0.1313 - acc: 0.950 - ETA: 1s - loss: 0.1360 - acc: 0.946 - ETA: 1s - loss: 0.1378 - acc: 0.950 - ETA: 0s - loss: 0.1485 - acc: 0.947 - ETA: 0s - loss: 0.1494 - acc: 0.948 - ETA: 0s - loss: 0.1596 - acc: 0.945 - ETA: 0s - loss: 0.1561 - acc: 0.945 - ETA: 0s - loss: 0.1564 - acc: 0.944 - ETA: 0s - loss: 0.1565 - acc: 0.945 - 2s 2ms/step - loss: 0.1568 - acc: 0.9447 - val_loss: 0.2567 - val_acc: 0.9089\n",
      "Epoch 27/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0739 - acc: 0.980 - ETA: 1s - loss: 0.0899 - acc: 0.965 - ETA: 1s - loss: 0.1240 - acc: 0.956 - ETA: 1s - loss: 0.1358 - acc: 0.947 - ETA: 1s - loss: 0.1404 - acc: 0.948 - ETA: 1s - loss: 0.1426 - acc: 0.946 - ETA: 0s - loss: 0.1452 - acc: 0.944 - ETA: 0s - loss: 0.1435 - acc: 0.945 - ETA: 0s - loss: 0.1541 - acc: 0.943 - ETA: 0s - loss: 0.1573 - acc: 0.942 - ETA: 0s - loss: 0.1650 - acc: 0.940 - ETA: 0s - loss: 0.1602 - acc: 0.943 - 2s 2ms/step - loss: 0.1567 - acc: 0.9447 - val_loss: 0.2297 - val_acc: 0.9124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0784 - acc: 0.980 - ETA: 1s - loss: 0.0991 - acc: 0.975 - ETA: 1s - loss: 0.1044 - acc: 0.973 - ETA: 1s - loss: 0.1171 - acc: 0.967 - ETA: 1s - loss: 0.1082 - acc: 0.970 - ETA: 1s - loss: 0.1217 - acc: 0.961 - ETA: 0s - loss: 0.1107 - acc: 0.967 - ETA: 0s - loss: 0.1196 - acc: 0.962 - ETA: 0s - loss: 0.1252 - acc: 0.962 - ETA: 0s - loss: 0.1284 - acc: 0.960 - ETA: 0s - loss: 0.1247 - acc: 0.960 - ETA: 0s - loss: 0.1241 - acc: 0.959 - 2s 2ms/step - loss: 0.1241 - acc: 0.9595 - val_loss: 0.2580 - val_acc: 0.9089\n",
      "Epoch 29/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0910 - acc: 0.970 - ETA: 1s - loss: 0.1045 - acc: 0.965 - ETA: 1s - loss: 0.1201 - acc: 0.960 - ETA: 1s - loss: 0.1047 - acc: 0.970 - ETA: 1s - loss: 0.0944 - acc: 0.974 - ETA: 1s - loss: 0.1166 - acc: 0.963 - ETA: 0s - loss: 0.1267 - acc: 0.962 - ETA: 0s - loss: 0.1210 - acc: 0.965 - ETA: 0s - loss: 0.1173 - acc: 0.964 - ETA: 0s - loss: 0.1191 - acc: 0.962 - ETA: 0s - loss: 0.1168 - acc: 0.963 - ETA: 0s - loss: 0.1181 - acc: 0.963 - 2s 2ms/step - loss: 0.1157 - acc: 0.9641 - val_loss: 0.2712 - val_acc: 0.9077\n",
      "Epoch 30/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.1338 - acc: 0.960 - ETA: 1s - loss: 0.1454 - acc: 0.950 - ETA: 1s - loss: 0.1535 - acc: 0.950 - ETA: 1s - loss: 0.1556 - acc: 0.942 - ETA: 1s - loss: 0.1564 - acc: 0.940 - ETA: 1s - loss: 0.1472 - acc: 0.943 - ETA: 1s - loss: 0.1481 - acc: 0.944 - ETA: 0s - loss: 0.1437 - acc: 0.947 - ETA: 0s - loss: 0.1433 - acc: 0.945 - ETA: 0s - loss: 0.1375 - acc: 0.948 - ETA: 0s - loss: 0.1373 - acc: 0.950 - ETA: 0s - loss: 0.1376 - acc: 0.950 - 3s 2ms/step - loss: 0.1406 - acc: 0.9501 - val_loss: 0.2585 - val_acc: 0.9159\n",
      "Epoch 31/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.1380 - acc: 0.970 - ETA: 1s - loss: 0.1057 - acc: 0.970 - ETA: 1s - loss: 0.1270 - acc: 0.963 - ETA: 1s - loss: 0.1269 - acc: 0.960 - ETA: 1s - loss: 0.1287 - acc: 0.954 - ETA: 1s - loss: 0.1305 - acc: 0.958 - ETA: 1s - loss: 0.1295 - acc: 0.955 - ETA: 0s - loss: 0.1226 - acc: 0.957 - ETA: 0s - loss: 0.1201 - acc: 0.958 - ETA: 0s - loss: 0.1158 - acc: 0.961 - ETA: 0s - loss: 0.1113 - acc: 0.963 - ETA: 0s - loss: 0.1126 - acc: 0.964 - 3s 2ms/step - loss: 0.1110 - acc: 0.9649 - val_loss: 0.2683 - val_acc: 0.9077\n",
      "Epoch 32/50\n",
      "1283/1283 [==============================] - ETA: 2s - loss: 0.0761 - acc: 0.970 - ETA: 1s - loss: 0.0905 - acc: 0.965 - ETA: 1s - loss: 0.0867 - acc: 0.966 - ETA: 1s - loss: 0.0886 - acc: 0.970 - ETA: 1s - loss: 0.0894 - acc: 0.966 - ETA: 1s - loss: 0.0910 - acc: 0.963 - ETA: 0s - loss: 0.1009 - acc: 0.961 - ETA: 0s - loss: 0.1088 - acc: 0.958 - ETA: 0s - loss: 0.1088 - acc: 0.960 - ETA: 0s - loss: 0.1141 - acc: 0.959 - ETA: 0s - loss: 0.1155 - acc: 0.958 - ETA: 0s - loss: 0.1240 - acc: 0.956 - 2s 2ms/step - loss: 0.1249 - acc: 0.9548 - val_loss: 0.2663 - val_acc: 0.9089\n",
      "Epoch 33/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0684 - acc: 0.980 - ETA: 1s - loss: 0.1003 - acc: 0.965 - ETA: 1s - loss: 0.1031 - acc: 0.963 - ETA: 1s - loss: 0.0974 - acc: 0.965 - ETA: 1s - loss: 0.0964 - acc: 0.962 - ETA: 1s - loss: 0.0982 - acc: 0.965 - ETA: 0s - loss: 0.1121 - acc: 0.960 - ETA: 0s - loss: 0.1221 - acc: 0.957 - ETA: 0s - loss: 0.1172 - acc: 0.961 - ETA: 0s - loss: 0.1137 - acc: 0.963 - ETA: 0s - loss: 0.1112 - acc: 0.961 - ETA: 0s - loss: 0.1109 - acc: 0.961 - 2s 2ms/step - loss: 0.1107 - acc: 0.9618 - val_loss: 0.2949 - val_acc: 0.9054\n",
      "Epoch 34/50\n",
      "1283/1283 [==============================] - ETA: 2s - loss: 0.1186 - acc: 0.970 - ETA: 2s - loss: 0.1011 - acc: 0.965 - ETA: 1s - loss: 0.1020 - acc: 0.963 - ETA: 1s - loss: 0.1183 - acc: 0.960 - ETA: 1s - loss: 0.1131 - acc: 0.960 - ETA: 1s - loss: 0.1069 - acc: 0.961 - ETA: 0s - loss: 0.1101 - acc: 0.960 - ETA: 0s - loss: 0.1059 - acc: 0.962 - ETA: 0s - loss: 0.1080 - acc: 0.961 - ETA: 0s - loss: 0.1097 - acc: 0.961 - ETA: 0s - loss: 0.1050 - acc: 0.962 - ETA: 0s - loss: 0.1013 - acc: 0.964 - 3s 2ms/step - loss: 0.0984 - acc: 0.9649 - val_loss: 0.2871 - val_acc: 0.9112\n",
      "Epoch 35/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0803 - acc: 0.980 - ETA: 1s - loss: 0.1084 - acc: 0.960 - ETA: 1s - loss: 0.0938 - acc: 0.963 - ETA: 1s - loss: 0.0966 - acc: 0.965 - ETA: 1s - loss: 0.0959 - acc: 0.964 - ETA: 1s - loss: 0.1025 - acc: 0.960 - ETA: 0s - loss: 0.0970 - acc: 0.962 - ETA: 0s - loss: 0.1002 - acc: 0.963 - ETA: 0s - loss: 0.0948 - acc: 0.966 - ETA: 0s - loss: 0.0986 - acc: 0.967 - ETA: 0s - loss: 0.0971 - acc: 0.965 - ETA: 0s - loss: 0.1033 - acc: 0.963 - 2s 2ms/step - loss: 0.1008 - acc: 0.9649 - val_loss: 0.2861 - val_acc: 0.9065\n",
      "Epoch 36/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0882 - acc: 0.970 - ETA: 1s - loss: 0.0968 - acc: 0.965 - ETA: 1s - loss: 0.0958 - acc: 0.970 - ETA: 1s - loss: 0.0844 - acc: 0.972 - ETA: 1s - loss: 0.0852 - acc: 0.976 - ETA: 1s - loss: 0.0836 - acc: 0.976 - ETA: 0s - loss: 0.0809 - acc: 0.975 - ETA: 0s - loss: 0.0790 - acc: 0.976 - ETA: 0s - loss: 0.0791 - acc: 0.976 - ETA: 0s - loss: 0.0804 - acc: 0.973 - ETA: 0s - loss: 0.0878 - acc: 0.971 - ETA: 0s - loss: 0.0836 - acc: 0.973 - 2s 2ms/step - loss: 0.0811 - acc: 0.9743 - val_loss: 0.2896 - val_acc: 0.9124\n",
      "Epoch 37/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0519 - acc: 0.980 - ETA: 1s - loss: 0.1102 - acc: 0.970 - ETA: 1s - loss: 0.1770 - acc: 0.963 - ETA: 1s - loss: 0.1628 - acc: 0.962 - ETA: 1s - loss: 0.1450 - acc: 0.964 - ETA: 1s - loss: 0.1344 - acc: 0.965 - ETA: 0s - loss: 0.1241 - acc: 0.967 - ETA: 0s - loss: 0.1151 - acc: 0.970 - ETA: 0s - loss: 0.1093 - acc: 0.970 - ETA: 0s - loss: 0.1006 - acc: 0.973 - ETA: 0s - loss: 0.0952 - acc: 0.974 - ETA: 0s - loss: 0.1014 - acc: 0.974 - 2s 2ms/step - loss: 0.1021 - acc: 0.9735 - val_loss: 0.2905 - val_acc: 0.9136\n",
      "Epoch 38/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0690 - acc: 0.970 - ETA: 1s - loss: 0.0878 - acc: 0.970 - ETA: 1s - loss: 0.1031 - acc: 0.970 - ETA: 1s - loss: 0.1093 - acc: 0.965 - ETA: 1s - loss: 0.1051 - acc: 0.964 - ETA: 1s - loss: 0.0971 - acc: 0.966 - ETA: 0s - loss: 0.0882 - acc: 0.968 - ETA: 0s - loss: 0.0819 - acc: 0.971 - ETA: 0s - loss: 0.0815 - acc: 0.972 - ETA: 0s - loss: 0.0894 - acc: 0.970 - ETA: 0s - loss: 0.0866 - acc: 0.970 - ETA: 0s - loss: 0.0829 - acc: 0.971 - 3s 2ms/step - loss: 0.0838 - acc: 0.9712 - val_loss: 0.2926 - val_acc: 0.9089\n",
      "Epoch 39/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0715 - acc: 0.980 - ETA: 1s - loss: 0.0733 - acc: 0.980 - ETA: 1s - loss: 0.0677 - acc: 0.980 - ETA: 1s - loss: 0.0659 - acc: 0.980 - ETA: 1s - loss: 0.0543 - acc: 0.984 - ETA: 1s - loss: 0.0561 - acc: 0.983 - ETA: 0s - loss: 0.0517 - acc: 0.984 - ETA: 0s - loss: 0.0610 - acc: 0.981 - ETA: 0s - loss: 0.0716 - acc: 0.980 - ETA: 0s - loss: 0.0745 - acc: 0.978 - ETA: 0s - loss: 0.0756 - acc: 0.977 - ETA: 0s - loss: 0.0749 - acc: 0.977 - 2s 2ms/step - loss: 0.0777 - acc: 0.9758 - val_loss: 0.2945 - val_acc: 0.9065\n",
      "Epoch 40/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0915 - acc: 0.960 - ETA: 1s - loss: 0.0715 - acc: 0.980 - ETA: 1s - loss: 0.0702 - acc: 0.976 - ETA: 1s - loss: 0.0608 - acc: 0.980 - ETA: 1s - loss: 0.0545 - acc: 0.982 - ETA: 1s - loss: 0.0553 - acc: 0.981 - ETA: 0s - loss: 0.0670 - acc: 0.981 - ETA: 0s - loss: 0.0640 - acc: 0.981 - ETA: 0s - loss: 0.0617 - acc: 0.983 - ETA: 0s - loss: 0.0618 - acc: 0.983 - ETA: 0s - loss: 0.0580 - acc: 0.983 - ETA: 0s - loss: 0.0640 - acc: 0.980 - 2s 2ms/step - loss: 0.0685 - acc: 0.9790 - val_loss: 0.3076 - val_acc: 0.9112\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0798 - acc: 0.980 - ETA: 1s - loss: 0.0515 - acc: 0.985 - ETA: 1s - loss: 0.0458 - acc: 0.986 - ETA: 1s - loss: 0.0654 - acc: 0.982 - ETA: 1s - loss: 0.0640 - acc: 0.982 - ETA: 1s - loss: 0.0579 - acc: 0.983 - ETA: 0s - loss: 0.0586 - acc: 0.981 - ETA: 0s - loss: 0.0571 - acc: 0.981 - ETA: 0s - loss: 0.0543 - acc: 0.982 - ETA: 0s - loss: 0.0550 - acc: 0.982 - ETA: 0s - loss: 0.0536 - acc: 0.982 - ETA: 0s - loss: 0.0596 - acc: 0.981 - 3s 2ms/step - loss: 0.0662 - acc: 0.9813 - val_loss: 0.3218 - val_acc: 0.9089\n",
      "Epoch 42/50\n",
      "1283/1283 [==============================] - ETA: 2s - loss: 0.0776 - acc: 0.990 - ETA: 2s - loss: 0.0876 - acc: 0.975 - ETA: 1s - loss: 0.0739 - acc: 0.980 - ETA: 1s - loss: 0.0836 - acc: 0.975 - ETA: 1s - loss: 0.0754 - acc: 0.978 - ETA: 1s - loss: 0.0663 - acc: 0.981 - ETA: 1s - loss: 0.0592 - acc: 0.984 - ETA: 0s - loss: 0.0582 - acc: 0.985 - ETA: 0s - loss: 0.0549 - acc: 0.986 - ETA: 0s - loss: 0.0553 - acc: 0.986 - ETA: 0s - loss: 0.0584 - acc: 0.984 - ETA: 0s - loss: 0.0662 - acc: 0.981 - 3s 2ms/step - loss: 0.0661 - acc: 0.9805 - val_loss: 0.3284 - val_acc: 0.9136\n",
      "Epoch 43/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0300 - acc: 0.990 - ETA: 1s - loss: 0.0415 - acc: 0.990 - ETA: 1s - loss: 0.0474 - acc: 0.986 - ETA: 1s - loss: 0.0462 - acc: 0.987 - ETA: 1s - loss: 0.0479 - acc: 0.986 - ETA: 1s - loss: 0.0484 - acc: 0.985 - ETA: 0s - loss: 0.0554 - acc: 0.981 - ETA: 0s - loss: 0.0540 - acc: 0.981 - ETA: 0s - loss: 0.0512 - acc: 0.983 - ETA: 0s - loss: 0.0475 - acc: 0.985 - ETA: 0s - loss: 0.0463 - acc: 0.985 - ETA: 0s - loss: 0.0456 - acc: 0.986 - 2s 2ms/step - loss: 0.0444 - acc: 0.9875 - val_loss: 0.3146 - val_acc: 0.9182\n",
      "Epoch 44/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0880 - acc: 0.990 - ETA: 1s - loss: 0.0623 - acc: 0.990 - ETA: 1s - loss: 0.0513 - acc: 0.993 - ETA: 1s - loss: 0.0510 - acc: 0.992 - ETA: 1s - loss: 0.0498 - acc: 0.990 - ETA: 1s - loss: 0.0584 - acc: 0.986 - ETA: 0s - loss: 0.0562 - acc: 0.985 - ETA: 0s - loss: 0.0654 - acc: 0.983 - ETA: 0s - loss: 0.0677 - acc: 0.982 - ETA: 0s - loss: 0.0691 - acc: 0.981 - ETA: 0s - loss: 0.0671 - acc: 0.980 - ETA: 0s - loss: 0.0633 - acc: 0.981 - 2s 2ms/step - loss: 0.0645 - acc: 0.9805 - val_loss: 0.3210 - val_acc: 0.9136\n",
      "Epoch 45/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0751 - acc: 0.980 - ETA: 1s - loss: 0.1017 - acc: 0.965 - ETA: 1s - loss: 0.0790 - acc: 0.973 - ETA: 1s - loss: 0.0743 - acc: 0.975 - ETA: 1s - loss: 0.0754 - acc: 0.972 - ETA: 1s - loss: 0.0658 - acc: 0.976 - ETA: 0s - loss: 0.0597 - acc: 0.980 - ETA: 0s - loss: 0.0595 - acc: 0.978 - ETA: 0s - loss: 0.0582 - acc: 0.980 - ETA: 0s - loss: 0.0569 - acc: 0.980 - ETA: 0s - loss: 0.0662 - acc: 0.978 - ETA: 0s - loss: 0.0620 - acc: 0.980 - 3s 2ms/step - loss: 0.0629 - acc: 0.9790 - val_loss: 0.3245 - val_acc: 0.9100\n",
      "Epoch 46/50\n",
      "1283/1283 [==============================] - ETA: 2s - loss: 0.1039 - acc: 0.970 - ETA: 2s - loss: 0.0863 - acc: 0.980 - ETA: 1s - loss: 0.0773 - acc: 0.980 - ETA: 1s - loss: 0.0693 - acc: 0.982 - ETA: 1s - loss: 0.0673 - acc: 0.982 - ETA: 1s - loss: 0.0696 - acc: 0.981 - ETA: 0s - loss: 0.0668 - acc: 0.981 - ETA: 0s - loss: 0.0625 - acc: 0.982 - ETA: 0s - loss: 0.0567 - acc: 0.984 - ETA: 0s - loss: 0.0537 - acc: 0.986 - ETA: 0s - loss: 0.0500 - acc: 0.987 - ETA: 0s - loss: 0.0490 - acc: 0.987 - 2s 2ms/step - loss: 0.0462 - acc: 0.9883 - val_loss: 0.3438 - val_acc: 0.9112\n",
      "Epoch 47/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0274 - acc: 0.990 - ETA: 1s - loss: 0.0265 - acc: 0.995 - ETA: 1s - loss: 0.0248 - acc: 0.996 - ETA: 1s - loss: 0.0282 - acc: 0.992 - ETA: 1s - loss: 0.0403 - acc: 0.990 - ETA: 1s - loss: 0.0370 - acc: 0.991 - ETA: 0s - loss: 0.0352 - acc: 0.991 - ETA: 0s - loss: 0.0398 - acc: 0.990 - ETA: 0s - loss: 0.0409 - acc: 0.988 - ETA: 0s - loss: 0.0448 - acc: 0.987 - ETA: 0s - loss: 0.0429 - acc: 0.988 - ETA: 0s - loss: 0.0421 - acc: 0.988 - 2s 2ms/step - loss: 0.0427 - acc: 0.9883 - val_loss: 0.3996 - val_acc: 0.9077\n",
      "Epoch 48/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0273 - acc: 1.000 - ETA: 1s - loss: 0.0356 - acc: 0.995 - ETA: 1s - loss: 0.0305 - acc: 0.993 - ETA: 1s - loss: 0.0419 - acc: 0.992 - ETA: 1s - loss: 0.0395 - acc: 0.994 - ETA: 1s - loss: 0.0422 - acc: 0.993 - ETA: 0s - loss: 0.0442 - acc: 0.988 - ETA: 0s - loss: 0.0414 - acc: 0.990 - ETA: 0s - loss: 0.0479 - acc: 0.988 - ETA: 0s - loss: 0.0470 - acc: 0.989 - ETA: 0s - loss: 0.0446 - acc: 0.989 - ETA: 0s - loss: 0.0439 - acc: 0.989 - 2s 2ms/step - loss: 0.0424 - acc: 0.9891 - val_loss: 0.3891 - val_acc: 0.9030\n",
      "Epoch 49/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0249 - acc: 0.990 - ETA: 1s - loss: 0.0379 - acc: 0.980 - ETA: 1s - loss: 0.0426 - acc: 0.983 - ETA: 1s - loss: 0.0328 - acc: 0.987 - ETA: 1s - loss: 0.0348 - acc: 0.986 - ETA: 1s - loss: 0.0363 - acc: 0.986 - ETA: 0s - loss: 0.0325 - acc: 0.988 - ETA: 0s - loss: 0.0378 - acc: 0.986 - ETA: 0s - loss: 0.0361 - acc: 0.986 - ETA: 0s - loss: 0.0355 - acc: 0.986 - ETA: 0s - loss: 0.0378 - acc: 0.985 - ETA: 0s - loss: 0.0381 - acc: 0.985 - 2s 2ms/step - loss: 0.0382 - acc: 0.9852 - val_loss: 0.3655 - val_acc: 0.8995\n",
      "Epoch 50/50\n",
      "1283/1283 [==============================] - ETA: 1s - loss: 0.0339 - acc: 0.990 - ETA: 1s - loss: 0.0350 - acc: 0.990 - ETA: 1s - loss: 0.0300 - acc: 0.990 - ETA: 1s - loss: 0.0344 - acc: 0.987 - ETA: 1s - loss: 0.0302 - acc: 0.990 - ETA: 1s - loss: 0.0316 - acc: 0.990 - ETA: 0s - loss: 0.0360 - acc: 0.985 - ETA: 0s - loss: 0.0403 - acc: 0.985 - ETA: 0s - loss: 0.0394 - acc: 0.984 - ETA: 0s - loss: 0.0437 - acc: 0.984 - ETA: 0s - loss: 0.0410 - acc: 0.985 - ETA: 0s - loss: 0.0415 - acc: 0.985 - 3s 2ms/step - loss: 0.0415 - acc: 0.9860 - val_loss: 0.3871 - val_acc: 0.8995\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "history = model.fit(X_train, y_train_hot, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(X_test, y_test_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export current model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to folder ./models/20181212_15-44-46\n"
     ]
    }
   ],
   "source": [
    "#export_model(model)\n",
    "\n",
    "y_predicted = model.predict_classes(X_test, batch_size=batch_size)\n",
    "y_true_val = np.argmax(y_test_hot,axis=1)\n",
    "\n",
    "class_rep = classification_report(y_true_val,y_predicted,digits=5)\n",
    "\n",
    "settings = {\n",
    "    \"feature_dim_1\": feature_dim_1,\n",
    "    \"feature_dim_2\": feature_dim_2,\n",
    "    \"channel\": channel,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"train_accuracy\": str(history.history.get('acc')[-1]),\n",
    "    \"test_accuracy\": str(history.history.get('val_acc')[-1]),\n",
    "    \"train_loss\": str(history.history.get('loss')[-1]),\n",
    "    \"test_loss\": str(history.history.get('val_loss')[-1]),\n",
    "    \"classification_report\": class_rep,\n",
    "}\n",
    "\n",
    "print(export_model(model, settings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_model(PATH)\n",
    "imported_model = import_model(\"./models/xxx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on a new file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(predict('./test_audio/12.wav', model=model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on a folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_PATH = './test_audio/'\n",
    "\n",
    "for filename in os.listdir(FOLDER_PATH):\n",
    "    pred = predict(FOLDER_PATH + filename, model=model)\n",
    "    print(filename + \" was predicted as: \" + pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy / Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Accuracy plot\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#Loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full report with confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#should import labels directly from folder:\n",
    "labels, _, _= get_labels(\"./audio\")\n",
    "labArray = []\n",
    "for label in labels:\n",
    "    labArray.append(label)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit_transform(labArray)\n",
    "\n",
    "full_multiclass_report(model, X_test, y_test_hot, classes=le.inverse_transform(np.arange(12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for all the metrics and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title='Normalized confusion matrix'\n",
    "    else:\n",
    "        title='Confusion matrix'\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "def full_multiclass_report(model,\n",
    "                           x,\n",
    "                           y_true,\n",
    "                           classes,\n",
    "                           batch_size=32,\n",
    "                           binary=False):\n",
    "\n",
    "    # 1. Transform one-hot encoded y_true into their class number\n",
    "    if not binary:\n",
    "        y_true = np.argmax(y_true,axis=1)\n",
    "    \n",
    "    # 2. Predict classes and stores in y_pred\n",
    "    y_pred = model.predict_classes(x, batch_size=batch_size)\n",
    "    \n",
    "    # 3. Print accuracy score\n",
    "    print(\"Accuracy : \"+ str(accuracy_score(y_true,y_pred)))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    # 4. Print classification report\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_true,y_pred,digits=5))    \n",
    "    \n",
    "    # 5. Plot confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_true,y_pred)\n",
    "    print(cnf_matrix)\n",
    "    plot_confusion_matrix(cnf_matrix,classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Accuracy\n",
    "    plt.figure(2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
