{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import IPython.display as ipd\n",
    "\n",
    "for filename in os.listdir('./test_audio'):\n",
    "    ipd.Audio('./test_audio/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "for filename in os.listdir('./test'):\n",
    "    data, sampling_rate = librosa.load('./test/' + filename)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = sr.AudioFile('0_jackson_2.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zero as source:\n",
    "    audio = r.record(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.recognize_google(audio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the main training part:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training with audio files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports:\n",
    "\n",
    "from export_model import *\n",
    "from preprocess import *\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving vectors of label - '1': 100%|█████████████████████████████████████████████████| 422/422 [00:10<00:00, 40.01it/s]\n",
      "Saving vectors of label - '10': 100%|████████████████████████████████████████████████| 258/258 [00:06<00:00, 42.91it/s]\n",
      "Saving vectors of label - '11': 100%|████████████████████████████████████████████████| 245/245 [00:05<00:00, 43.41it/s]\n",
      "Saving vectors of label - '12': 100%|████████████████████████████████████████████████| 222/222 [00:04<00:00, 45.53it/s]\n",
      "Saving vectors of label - '2': 100%|█████████████████████████████████████████████████| 371/371 [00:08<00:00, 44.46it/s]\n",
      "Saving vectors of label - '3': 100%|█████████████████████████████████████████████████| 359/359 [00:08<00:00, 43.19it/s]\n",
      "Saving vectors of label - '4': 100%|█████████████████████████████████████████████████| 230/230 [00:05<00:00, 39.61it/s]\n",
      "Saving vectors of label - '5': 100%|█████████████████████████████████████████████████| 262/262 [00:05<00:00, 48.71it/s]\n",
      "Saving vectors of label - '6': 100%|█████████████████████████████████████████████████| 235/235 [00:05<00:00, 44.45it/s]\n",
      "Saving vectors of label - '7': 100%|█████████████████████████████████████████████████| 255/255 [00:05<00:00, 46.04it/s]\n",
      "Saving vectors of label - '8': 100%|█████████████████████████████████████████████████| 250/250 [00:05<00:00, 46.02it/s]\n",
      "Saving vectors of label - '9': 100%|█████████████████████████████████████████████████| 245/245 [00:05<00:00, 39.45it/s]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#from preprocess import *\n",
    "#import keras\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "#from keras.utils import to_categorical\n",
    "#from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#import itertools\n",
    "\n",
    "# Second dimension of the feature is dim2\n",
    "feature_dim_2 = 11\n",
    "\n",
    "# Save data to array file first\n",
    "save_data_to_array(max_len=feature_dim_2)\n",
    "\n",
    "# # Loading train set and test set\n",
    "X_train, X_test, y_train, y_test = get_train_test()\n",
    "\n",
    "# # Feature dimension\n",
    "#defaults at the end\n",
    "feature_dim_1 = 20   #20\n",
    "channel = 1          #1\n",
    "epochs = 50         #50\n",
    "batch_size = 100     #100\n",
    "verbose = 1          #1\n",
    "# change num_classes depending on the amount of labels\n",
    "num_classes = 12\n",
    "\n",
    "# Reshaping to perform 2D convolution\n",
    "X_train = X_train.reshape(X_train.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "X_test = X_test.reshape(X_test.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model & prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(feature_dim_1, feature_dim_2, channel)))\n",
    "    model.add(Conv2D(48, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(120, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Predicts one sample\n",
    "def predict(filepath, model):\n",
    "    sample = wav2mfcc(filepath)\n",
    "    sample_reshaped = sample.reshape(1, feature_dim_1, feature_dim_2, channel)\n",
    "    return get_labels()[0][\n",
    "            np.argmax(model.predict(sample_reshaped))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2012 samples, validate on 1342 samples\n",
      "Epoch 1/50\n",
      "2012/2012 [==============================] - ETA: 15s - loss: 6.1324 - acc: 0.10 - ETA: 8s - loss: 6.7553 - acc: 0.1050 - ETA: 5s - loss: 6.6199 - acc: 0.096 - ETA: 4s - loss: 5.9644 - acc: 0.102 - ETA: 3s - loss: 5.4231 - acc: 0.096 - ETA: 3s - loss: 4.9878 - acc: 0.105 - ETA: 2s - loss: 4.6502 - acc: 0.107 - ETA: 2s - loss: 4.3799 - acc: 0.108 - ETA: 2s - loss: 4.1534 - acc: 0.120 - ETA: 1s - loss: 3.9713 - acc: 0.126 - ETA: 1s - loss: 3.8348 - acc: 0.125 - ETA: 1s - loss: 3.7200 - acc: 0.127 - ETA: 1s - loss: 3.6064 - acc: 0.132 - ETA: 0s - loss: 3.5143 - acc: 0.135 - ETA: 0s - loss: 3.4387 - acc: 0.134 - ETA: 0s - loss: 3.3635 - acc: 0.136 - ETA: 0s - loss: 3.2942 - acc: 0.145 - ETA: 0s - loss: 3.2507 - acc: 0.147 - ETA: 0s - loss: 3.1969 - acc: 0.147 - ETA: 0s - loss: 3.1483 - acc: 0.150 - 3s 2ms/step - loss: 3.1409 - acc: 0.1521 - val_loss: 1.8997 - val_acc: 0.4009\n",
      "Epoch 2/50\n",
      "2012/2012 [==============================] - ETA: 1s - loss: 2.3347 - acc: 0.230 - ETA: 1s - loss: 2.3013 - acc: 0.205 - ETA: 1s - loss: 2.2522 - acc: 0.230 - ETA: 1s - loss: 2.1716 - acc: 0.257 - ETA: 1s - loss: 2.1291 - acc: 0.266 - ETA: 1s - loss: 2.1017 - acc: 0.273 - ETA: 1s - loss: 2.0711 - acc: 0.280 - ETA: 1s - loss: 2.0431 - acc: 0.288 - ETA: 1s - loss: 2.0246 - acc: 0.292 - ETA: 1s - loss: 2.0091 - acc: 0.302 - ETA: 0s - loss: 1.9839 - acc: 0.315 - ETA: 0s - loss: 1.9703 - acc: 0.324 - ETA: 0s - loss: 1.9681 - acc: 0.326 - ETA: 0s - loss: 1.9564 - acc: 0.335 - ETA: 0s - loss: 1.9315 - acc: 0.344 - ETA: 0s - loss: 1.9195 - acc: 0.348 - ETA: 0s - loss: 1.9086 - acc: 0.351 - ETA: 0s - loss: 1.8975 - acc: 0.356 - ETA: 0s - loss: 1.8743 - acc: 0.365 - ETA: 0s - loss: 1.8699 - acc: 0.368 - 3s 1ms/step - loss: 1.8649 - acc: 0.3698 - val_loss: 1.2341 - val_acc: 0.6349\n",
      "Epoch 3/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 1.7526 - acc: 0.380 - ETA: 1s - loss: 1.6975 - acc: 0.390 - ETA: 1s - loss: 1.6411 - acc: 0.443 - ETA: 1s - loss: 1.5835 - acc: 0.455 - ETA: 1s - loss: 1.5603 - acc: 0.468 - ETA: 1s - loss: 1.5290 - acc: 0.481 - ETA: 1s - loss: 1.4992 - acc: 0.498 - ETA: 1s - loss: 1.4692 - acc: 0.511 - ETA: 1s - loss: 1.4522 - acc: 0.510 - ETA: 1s - loss: 1.4497 - acc: 0.514 - ETA: 0s - loss: 1.4376 - acc: 0.521 - ETA: 0s - loss: 1.4213 - acc: 0.529 - ETA: 0s - loss: 1.4108 - acc: 0.529 - ETA: 0s - loss: 1.4196 - acc: 0.530 - ETA: 0s - loss: 1.4136 - acc: 0.530 - ETA: 0s - loss: 1.4067 - acc: 0.532 - ETA: 0s - loss: 1.4077 - acc: 0.532 - ETA: 0s - loss: 1.3969 - acc: 0.536 - ETA: 0s - loss: 1.3756 - acc: 0.543 - ETA: 0s - loss: 1.3615 - acc: 0.546 - 3s 1ms/step - loss: 1.3588 - acc: 0.5477 - val_loss: 1.1154 - val_acc: 0.6304\n",
      "Epoch 4/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 1.7164 - acc: 0.500 - ETA: 2s - loss: 1.5915 - acc: 0.510 - ETA: 2s - loss: 1.4599 - acc: 0.550 - ETA: 1s - loss: 1.3726 - acc: 0.577 - ETA: 1s - loss: 1.3008 - acc: 0.588 - ETA: 1s - loss: 1.2505 - acc: 0.605 - ETA: 1s - loss: 1.2315 - acc: 0.605 - ETA: 1s - loss: 1.1846 - acc: 0.618 - ETA: 1s - loss: 1.1534 - acc: 0.627 - ETA: 1s - loss: 1.1313 - acc: 0.637 - ETA: 1s - loss: 1.0976 - acc: 0.647 - ETA: 0s - loss: 1.0900 - acc: 0.651 - ETA: 0s - loss: 1.0834 - acc: 0.652 - ETA: 0s - loss: 1.0680 - acc: 0.655 - ETA: 0s - loss: 1.0485 - acc: 0.662 - ETA: 0s - loss: 1.0345 - acc: 0.666 - ETA: 0s - loss: 1.0263 - acc: 0.670 - ETA: 0s - loss: 1.0205 - acc: 0.670 - ETA: 0s - loss: 1.0078 - acc: 0.674 - ETA: 0s - loss: 1.0017 - acc: 0.676 - 3s 1ms/step - loss: 0.9998 - acc: 0.6769 - val_loss: 0.5904 - val_acc: 0.8241\n",
      "Epoch 5/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.8682 - acc: 0.660 - ETA: 1s - loss: 0.8274 - acc: 0.695 - ETA: 1s - loss: 0.7735 - acc: 0.720 - ETA: 1s - loss: 0.7939 - acc: 0.720 - ETA: 1s - loss: 0.7665 - acc: 0.730 - ETA: 1s - loss: 0.8033 - acc: 0.716 - ETA: 1s - loss: 0.7898 - acc: 0.730 - ETA: 1s - loss: 0.7868 - acc: 0.731 - ETA: 1s - loss: 0.7792 - acc: 0.735 - ETA: 1s - loss: 0.7720 - acc: 0.737 - ETA: 1s - loss: 0.7746 - acc: 0.732 - ETA: 0s - loss: 0.7771 - acc: 0.730 - ETA: 0s - loss: 0.7642 - acc: 0.732 - ETA: 0s - loss: 0.7539 - acc: 0.737 - ETA: 0s - loss: 0.7508 - acc: 0.738 - ETA: 0s - loss: 0.7410 - acc: 0.740 - ETA: 0s - loss: 0.7327 - acc: 0.743 - ETA: 0s - loss: 0.7288 - acc: 0.745 - ETA: 0s - loss: 0.7349 - acc: 0.744 - ETA: 0s - loss: 0.7442 - acc: 0.739 - 3s 1ms/step - loss: 0.7437 - acc: 0.7396 - val_loss: 0.5449 - val_acc: 0.8167\n",
      "Epoch 6/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.6364 - acc: 0.790 - ETA: 1s - loss: 0.6568 - acc: 0.790 - ETA: 1s - loss: 0.6331 - acc: 0.796 - ETA: 1s - loss: 0.6268 - acc: 0.792 - ETA: 1s - loss: 0.6786 - acc: 0.780 - ETA: 1s - loss: 0.6733 - acc: 0.780 - ETA: 1s - loss: 0.6432 - acc: 0.791 - ETA: 1s - loss: 0.6650 - acc: 0.786 - ETA: 1s - loss: 0.6575 - acc: 0.788 - ETA: 1s - loss: 0.6478 - acc: 0.791 - ETA: 1s - loss: 0.6309 - acc: 0.797 - ETA: 0s - loss: 0.6421 - acc: 0.796 - ETA: 0s - loss: 0.6381 - acc: 0.796 - ETA: 0s - loss: 0.6293 - acc: 0.800 - ETA: 0s - loss: 0.6210 - acc: 0.803 - ETA: 0s - loss: 0.6169 - acc: 0.804 - ETA: 0s - loss: 0.6163 - acc: 0.805 - ETA: 0s - loss: 0.6211 - acc: 0.802 - ETA: 0s - loss: 0.6163 - acc: 0.803 - ETA: 0s - loss: 0.6102 - acc: 0.805 - 3s 1ms/step - loss: 0.6101 - acc: 0.8057 - val_loss: 0.4173 - val_acc: 0.8614\n",
      "Epoch 7/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.7654 - acc: 0.740 - ETA: 2s - loss: 0.6883 - acc: 0.770 - ETA: 2s - loss: 0.6460 - acc: 0.790 - ETA: 2s - loss: 0.6268 - acc: 0.790 - ETA: 1s - loss: 0.5948 - acc: 0.800 - ETA: 1s - loss: 0.5836 - acc: 0.801 - ETA: 1s - loss: 0.5772 - acc: 0.801 - ETA: 1s - loss: 0.5769 - acc: 0.802 - ETA: 1s - loss: 0.5709 - acc: 0.805 - ETA: 1s - loss: 0.5803 - acc: 0.798 - ETA: 1s - loss: 0.5731 - acc: 0.800 - ETA: 0s - loss: 0.5592 - acc: 0.807 - ETA: 0s - loss: 0.5583 - acc: 0.803 - ETA: 0s - loss: 0.5530 - acc: 0.805 - ETA: 0s - loss: 0.5457 - acc: 0.806 - ETA: 0s - loss: 0.5478 - acc: 0.808 - ETA: 0s - loss: 0.5411 - acc: 0.812 - ETA: 0s - loss: 0.5389 - acc: 0.812 - ETA: 0s - loss: 0.5410 - acc: 0.812 - ETA: 0s - loss: 0.5387 - acc: 0.815 - 3s 1ms/step - loss: 0.5362 - acc: 0.8156 - val_loss: 0.3548 - val_acc: 0.8815\n",
      "Epoch 8/50\n",
      "2012/2012 [==============================] - ETA: 1s - loss: 0.5867 - acc: 0.820 - ETA: 1s - loss: 0.5170 - acc: 0.830 - ETA: 1s - loss: 0.5503 - acc: 0.820 - ETA: 1s - loss: 0.5533 - acc: 0.810 - ETA: 1s - loss: 0.5526 - acc: 0.802 - ETA: 1s - loss: 0.5198 - acc: 0.815 - ETA: 1s - loss: 0.5290 - acc: 0.810 - ETA: 1s - loss: 0.5491 - acc: 0.800 - ETA: 1s - loss: 0.5489 - acc: 0.802 - ETA: 1s - loss: 0.5329 - acc: 0.812 - ETA: 1s - loss: 0.5296 - acc: 0.813 - ETA: 0s - loss: 0.5162 - acc: 0.818 - ETA: 0s - loss: 0.5125 - acc: 0.819 - ETA: 0s - loss: 0.5030 - acc: 0.822 - ETA: 0s - loss: 0.4848 - acc: 0.829 - ETA: 0s - loss: 0.4787 - acc: 0.831 - ETA: 0s - loss: 0.4726 - acc: 0.835 - ETA: 0s - loss: 0.4719 - acc: 0.836 - ETA: 0s - loss: 0.4714 - acc: 0.837 - ETA: 0s - loss: 0.4687 - acc: 0.839 - 3s 1ms/step - loss: 0.4672 - acc: 0.8395 - val_loss: 0.3732 - val_acc: 0.8726\n",
      "Epoch 9/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.4412 - acc: 0.860 - ETA: 1s - loss: 0.4711 - acc: 0.850 - ETA: 2s - loss: 0.4434 - acc: 0.856 - ETA: 1s - loss: 0.4337 - acc: 0.857 - ETA: 1s - loss: 0.4503 - acc: 0.844 - ETA: 1s - loss: 0.4612 - acc: 0.840 - ETA: 1s - loss: 0.4805 - acc: 0.831 - ETA: 1s - loss: 0.4658 - acc: 0.836 - ETA: 1s - loss: 0.4749 - acc: 0.836 - ETA: 1s - loss: 0.4646 - acc: 0.843 - ETA: 1s - loss: 0.4612 - acc: 0.844 - ETA: 0s - loss: 0.4600 - acc: 0.845 - ETA: 0s - loss: 0.4518 - acc: 0.846 - ETA: 0s - loss: 0.4454 - acc: 0.848 - ETA: 0s - loss: 0.4452 - acc: 0.848 - ETA: 0s - loss: 0.4420 - acc: 0.848 - ETA: 0s - loss: 0.4393 - acc: 0.848 - ETA: 0s - loss: 0.4450 - acc: 0.844 - ETA: 0s - loss: 0.4346 - acc: 0.848 - ETA: 0s - loss: 0.4286 - acc: 0.851 - 3s 1ms/step - loss: 0.4293 - acc: 0.8509 - val_loss: 0.3888 - val_acc: 0.8674\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012/2012 [==============================] - ETA: 2s - loss: 0.5298 - acc: 0.800 - ETA: 1s - loss: 0.4823 - acc: 0.825 - ETA: 1s - loss: 0.4897 - acc: 0.826 - ETA: 1s - loss: 0.4767 - acc: 0.832 - ETA: 1s - loss: 0.4634 - acc: 0.838 - ETA: 1s - loss: 0.4779 - acc: 0.831 - ETA: 1s - loss: 0.4783 - acc: 0.830 - ETA: 1s - loss: 0.4721 - acc: 0.833 - ETA: 1s - loss: 0.4526 - acc: 0.836 - ETA: 1s - loss: 0.4439 - acc: 0.838 - ETA: 1s - loss: 0.4441 - acc: 0.837 - ETA: 0s - loss: 0.4497 - acc: 0.833 - ETA: 0s - loss: 0.4548 - acc: 0.834 - ETA: 0s - loss: 0.4664 - acc: 0.831 - ETA: 0s - loss: 0.4638 - acc: 0.833 - ETA: 0s - loss: 0.4600 - acc: 0.833 - ETA: 0s - loss: 0.4504 - acc: 0.835 - ETA: 0s - loss: 0.4391 - acc: 0.841 - ETA: 0s - loss: 0.4489 - acc: 0.838 - ETA: 0s - loss: 0.4467 - acc: 0.839 - 3s 1ms/step - loss: 0.4451 - acc: 0.8400 - val_loss: 0.3146 - val_acc: 0.8845\n",
      "Epoch 11/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.4251 - acc: 0.850 - ETA: 2s - loss: 0.3673 - acc: 0.870 - ETA: 1s - loss: 0.3655 - acc: 0.873 - ETA: 1s - loss: 0.3935 - acc: 0.867 - ETA: 1s - loss: 0.3731 - acc: 0.874 - ETA: 1s - loss: 0.3743 - acc: 0.876 - ETA: 1s - loss: 0.3923 - acc: 0.870 - ETA: 1s - loss: 0.3884 - acc: 0.870 - ETA: 1s - loss: 0.3883 - acc: 0.868 - ETA: 1s - loss: 0.3843 - acc: 0.871 - ETA: 1s - loss: 0.3830 - acc: 0.872 - ETA: 0s - loss: 0.3838 - acc: 0.872 - ETA: 0s - loss: 0.3813 - acc: 0.873 - ETA: 0s - loss: 0.3831 - acc: 0.872 - ETA: 0s - loss: 0.3811 - acc: 0.872 - ETA: 0s - loss: 0.3805 - acc: 0.871 - ETA: 0s - loss: 0.3927 - acc: 0.866 - ETA: 0s - loss: 0.3927 - acc: 0.866 - ETA: 0s - loss: 0.3924 - acc: 0.866 - ETA: 0s - loss: 0.3885 - acc: 0.867 - 3s 1ms/step - loss: 0.3880 - acc: 0.8673 - val_loss: 0.3409 - val_acc: 0.8815\n",
      "Epoch 12/50\n",
      "2012/2012 [==============================] - ETA: 1s - loss: 0.4717 - acc: 0.860 - ETA: 1s - loss: 0.3819 - acc: 0.885 - ETA: 1s - loss: 0.3696 - acc: 0.886 - ETA: 1s - loss: 0.3452 - acc: 0.890 - ETA: 1s - loss: 0.3480 - acc: 0.890 - ETA: 1s - loss: 0.3303 - acc: 0.896 - ETA: 1s - loss: 0.3600 - acc: 0.887 - ETA: 1s - loss: 0.3505 - acc: 0.888 - ETA: 1s - loss: 0.3507 - acc: 0.884 - ETA: 1s - loss: 0.3460 - acc: 0.886 - ETA: 0s - loss: 0.3484 - acc: 0.881 - ETA: 0s - loss: 0.3577 - acc: 0.880 - ETA: 0s - loss: 0.3712 - acc: 0.872 - ETA: 0s - loss: 0.3618 - acc: 0.875 - ETA: 0s - loss: 0.3545 - acc: 0.879 - ETA: 0s - loss: 0.3597 - acc: 0.880 - ETA: 0s - loss: 0.3567 - acc: 0.880 - ETA: 0s - loss: 0.3536 - acc: 0.880 - ETA: 0s - loss: 0.3644 - acc: 0.876 - ETA: 0s - loss: 0.3587 - acc: 0.878 - 3s 1ms/step - loss: 0.3584 - acc: 0.8782 - val_loss: 0.3263 - val_acc: 0.8852\n",
      "Epoch 13/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.4697 - acc: 0.820 - ETA: 1s - loss: 0.4050 - acc: 0.845 - ETA: 1s - loss: 0.3376 - acc: 0.880 - ETA: 1s - loss: 0.3325 - acc: 0.885 - ETA: 1s - loss: 0.3345 - acc: 0.888 - ETA: 1s - loss: 0.3534 - acc: 0.883 - ETA: 1s - loss: 0.3623 - acc: 0.877 - ETA: 1s - loss: 0.3548 - acc: 0.881 - ETA: 1s - loss: 0.3404 - acc: 0.886 - ETA: 1s - loss: 0.3286 - acc: 0.893 - ETA: 0s - loss: 0.3224 - acc: 0.891 - ETA: 0s - loss: 0.3264 - acc: 0.890 - ETA: 0s - loss: 0.3246 - acc: 0.890 - ETA: 0s - loss: 0.3134 - acc: 0.892 - ETA: 0s - loss: 0.3104 - acc: 0.894 - ETA: 0s - loss: 0.3085 - acc: 0.893 - ETA: 0s - loss: 0.3066 - acc: 0.894 - ETA: 0s - loss: 0.3063 - acc: 0.893 - ETA: 0s - loss: 0.2971 - acc: 0.895 - ETA: 0s - loss: 0.2991 - acc: 0.896 - 3s 1ms/step - loss: 0.2994 - acc: 0.8961 - val_loss: 0.3434 - val_acc: 0.8852\n",
      "Epoch 14/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.3906 - acc: 0.890 - ETA: 1s - loss: 0.3279 - acc: 0.885 - ETA: 1s - loss: 0.3567 - acc: 0.866 - ETA: 1s - loss: 0.3642 - acc: 0.875 - ETA: 1s - loss: 0.3534 - acc: 0.884 - ETA: 1s - loss: 0.3457 - acc: 0.886 - ETA: 1s - loss: 0.3375 - acc: 0.885 - ETA: 1s - loss: 0.3290 - acc: 0.886 - ETA: 1s - loss: 0.3172 - acc: 0.891 - ETA: 1s - loss: 0.3108 - acc: 0.893 - ETA: 1s - loss: 0.3155 - acc: 0.887 - ETA: 0s - loss: 0.3067 - acc: 0.891 - ETA: 0s - loss: 0.2993 - acc: 0.893 - ETA: 0s - loss: 0.3032 - acc: 0.891 - ETA: 0s - loss: 0.2959 - acc: 0.892 - ETA: 0s - loss: 0.2984 - acc: 0.890 - ETA: 0s - loss: 0.2969 - acc: 0.891 - ETA: 0s - loss: 0.2938 - acc: 0.892 - ETA: 0s - loss: 0.2988 - acc: 0.890 - ETA: 0s - loss: 0.2919 - acc: 0.893 - 3s 1ms/step - loss: 0.2909 - acc: 0.8936 - val_loss: 0.3162 - val_acc: 0.8905\n",
      "Epoch 15/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.3349 - acc: 0.900 - ETA: 2s - loss: 0.2860 - acc: 0.905 - ETA: 2s - loss: 0.3208 - acc: 0.896 - ETA: 1s - loss: 0.2984 - acc: 0.902 - ETA: 1s - loss: 0.2897 - acc: 0.902 - ETA: 1s - loss: 0.2906 - acc: 0.900 - ETA: 1s - loss: 0.2918 - acc: 0.900 - ETA: 1s - loss: 0.2798 - acc: 0.903 - ETA: 1s - loss: 0.2802 - acc: 0.905 - ETA: 1s - loss: 0.2821 - acc: 0.903 - ETA: 1s - loss: 0.2803 - acc: 0.903 - ETA: 0s - loss: 0.2869 - acc: 0.900 - ETA: 0s - loss: 0.2829 - acc: 0.901 - ETA: 0s - loss: 0.2800 - acc: 0.902 - ETA: 0s - loss: 0.2762 - acc: 0.905 - ETA: 0s - loss: 0.2910 - acc: 0.900 - ETA: 0s - loss: 0.2916 - acc: 0.898 - ETA: 0s - loss: 0.2907 - acc: 0.898 - ETA: 0s - loss: 0.2833 - acc: 0.901 - ETA: 0s - loss: 0.2774 - acc: 0.903 - 3s 1ms/step - loss: 0.2762 - acc: 0.9031 - val_loss: 0.2794 - val_acc: 0.9061\n",
      "Epoch 16/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.3026 - acc: 0.900 - ETA: 1s - loss: 0.3171 - acc: 0.890 - ETA: 1s - loss: 0.3365 - acc: 0.886 - ETA: 1s - loss: 0.3211 - acc: 0.890 - ETA: 1s - loss: 0.3007 - acc: 0.894 - ETA: 1s - loss: 0.2935 - acc: 0.893 - ETA: 1s - loss: 0.2962 - acc: 0.897 - ETA: 1s - loss: 0.2899 - acc: 0.898 - ETA: 1s - loss: 0.2821 - acc: 0.902 - ETA: 1s - loss: 0.2767 - acc: 0.900 - ETA: 1s - loss: 0.2791 - acc: 0.899 - ETA: 0s - loss: 0.2711 - acc: 0.901 - ETA: 0s - loss: 0.2687 - acc: 0.902 - ETA: 0s - loss: 0.2660 - acc: 0.903 - ETA: 0s - loss: 0.2626 - acc: 0.903 - ETA: 0s - loss: 0.2689 - acc: 0.901 - ETA: 0s - loss: 0.2655 - acc: 0.902 - ETA: 0s - loss: 0.2641 - acc: 0.903 - ETA: 0s - loss: 0.2620 - acc: 0.904 - ETA: 0s - loss: 0.2575 - acc: 0.907 - 3s 1ms/step - loss: 0.2567 - acc: 0.9071 - val_loss: 0.3088 - val_acc: 0.8890\n",
      "Epoch 17/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.2425 - acc: 0.900 - ETA: 1s - loss: 0.2515 - acc: 0.900 - ETA: 1s - loss: 0.2475 - acc: 0.900 - ETA: 1s - loss: 0.2414 - acc: 0.915 - ETA: 1s - loss: 0.2478 - acc: 0.910 - ETA: 1s - loss: 0.2484 - acc: 0.913 - ETA: 1s - loss: 0.2368 - acc: 0.918 - ETA: 1s - loss: 0.2333 - acc: 0.918 - ETA: 1s - loss: 0.2290 - acc: 0.920 - ETA: 1s - loss: 0.2385 - acc: 0.916 - ETA: 1s - loss: 0.2383 - acc: 0.914 - ETA: 0s - loss: 0.2452 - acc: 0.913 - ETA: 0s - loss: 0.2463 - acc: 0.912 - ETA: 0s - loss: 0.2478 - acc: 0.911 - ETA: 0s - loss: 0.2511 - acc: 0.910 - ETA: 0s - loss: 0.2471 - acc: 0.910 - ETA: 0s - loss: 0.2471 - acc: 0.911 - ETA: 0s - loss: 0.2406 - acc: 0.914 - ETA: 0s - loss: 0.2433 - acc: 0.913 - ETA: 0s - loss: 0.2556 - acc: 0.909 - 3s 1ms/step - loss: 0.2558 - acc: 0.9095 - val_loss: 0.3181 - val_acc: 0.8912\n",
      "Epoch 18/50\n",
      "2012/2012 [==============================] - ETA: 1s - loss: 0.4964 - acc: 0.820 - ETA: 1s - loss: 0.3563 - acc: 0.860 - ETA: 1s - loss: 0.3121 - acc: 0.880 - ETA: 1s - loss: 0.2967 - acc: 0.882 - ETA: 1s - loss: 0.3008 - acc: 0.890 - ETA: 1s - loss: 0.2964 - acc: 0.893 - ETA: 1s - loss: 0.2766 - acc: 0.898 - ETA: 1s - loss: 0.2743 - acc: 0.897 - ETA: 1s - loss: 0.2912 - acc: 0.890 - ETA: 1s - loss: 0.2812 - acc: 0.893 - ETA: 1s - loss: 0.2760 - acc: 0.895 - ETA: 0s - loss: 0.2646 - acc: 0.900 - ETA: 0s - loss: 0.2690 - acc: 0.899 - ETA: 0s - loss: 0.2661 - acc: 0.900 - ETA: 0s - loss: 0.2686 - acc: 0.901 - ETA: 0s - loss: 0.2703 - acc: 0.901 - ETA: 0s - loss: 0.2628 - acc: 0.904 - ETA: 0s - loss: 0.2657 - acc: 0.903 - ETA: 0s - loss: 0.2702 - acc: 0.901 - ETA: 0s - loss: 0.2704 - acc: 0.902 - 3s 1ms/step - loss: 0.2691 - acc: 0.9026 - val_loss: 0.2814 - val_acc: 0.9046\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012/2012 [==============================] - ETA: 1s - loss: 0.1928 - acc: 0.920 - ETA: 1s - loss: 0.2611 - acc: 0.905 - ETA: 1s - loss: 0.2389 - acc: 0.910 - ETA: 1s - loss: 0.2223 - acc: 0.910 - ETA: 1s - loss: 0.2037 - acc: 0.918 - ETA: 1s - loss: 0.2122 - acc: 0.915 - ETA: 1s - loss: 0.2191 - acc: 0.914 - ETA: 1s - loss: 0.2254 - acc: 0.911 - ETA: 1s - loss: 0.2229 - acc: 0.912 - ETA: 1s - loss: 0.2333 - acc: 0.909 - ETA: 0s - loss: 0.2367 - acc: 0.909 - ETA: 0s - loss: 0.2292 - acc: 0.911 - ETA: 0s - loss: 0.2304 - acc: 0.914 - ETA: 0s - loss: 0.2227 - acc: 0.917 - ETA: 0s - loss: 0.2291 - acc: 0.914 - ETA: 0s - loss: 0.2316 - acc: 0.913 - ETA: 0s - loss: 0.2293 - acc: 0.914 - ETA: 0s - loss: 0.2262 - acc: 0.915 - ETA: 0s - loss: 0.2275 - acc: 0.914 - ETA: 0s - loss: 0.2261 - acc: 0.914 - 3s 1ms/step - loss: 0.2261 - acc: 0.9150 - val_loss: 0.2894 - val_acc: 0.9069\n",
      "Epoch 20/50\n",
      "2012/2012 [==============================] - ETA: 1s - loss: 0.2712 - acc: 0.890 - ETA: 1s - loss: 0.2398 - acc: 0.900 - ETA: 1s - loss: 0.2551 - acc: 0.903 - ETA: 1s - loss: 0.2417 - acc: 0.910 - ETA: 1s - loss: 0.2585 - acc: 0.906 - ETA: 1s - loss: 0.2625 - acc: 0.906 - ETA: 1s - loss: 0.2503 - acc: 0.910 - ETA: 1s - loss: 0.2467 - acc: 0.913 - ETA: 1s - loss: 0.2434 - acc: 0.913 - ETA: 1s - loss: 0.2471 - acc: 0.913 - ETA: 0s - loss: 0.2468 - acc: 0.913 - ETA: 0s - loss: 0.2502 - acc: 0.912 - ETA: 0s - loss: 0.2379 - acc: 0.916 - ETA: 0s - loss: 0.2322 - acc: 0.920 - ETA: 0s - loss: 0.2247 - acc: 0.922 - ETA: 0s - loss: 0.2252 - acc: 0.923 - ETA: 0s - loss: 0.2256 - acc: 0.922 - ETA: 0s - loss: 0.2244 - acc: 0.922 - ETA: 0s - loss: 0.2201 - acc: 0.924 - ETA: 0s - loss: 0.2234 - acc: 0.923 - 3s 1ms/step - loss: 0.2228 - acc: 0.9235 - val_loss: 0.2932 - val_acc: 0.9046\n",
      "Epoch 21/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.1175 - acc: 0.970 - ETA: 2s - loss: 0.1267 - acc: 0.955 - ETA: 1s - loss: 0.1309 - acc: 0.953 - ETA: 1s - loss: 0.1600 - acc: 0.942 - ETA: 1s - loss: 0.1717 - acc: 0.938 - ETA: 1s - loss: 0.1808 - acc: 0.933 - ETA: 1s - loss: 0.2047 - acc: 0.924 - ETA: 1s - loss: 0.1954 - acc: 0.925 - ETA: 1s - loss: 0.2192 - acc: 0.917 - ETA: 1s - loss: 0.2218 - acc: 0.915 - ETA: 1s - loss: 0.2294 - acc: 0.911 - ETA: 0s - loss: 0.2521 - acc: 0.909 - ETA: 0s - loss: 0.2563 - acc: 0.906 - ETA: 0s - loss: 0.2551 - acc: 0.905 - ETA: 0s - loss: 0.2508 - acc: 0.907 - ETA: 0s - loss: 0.2520 - acc: 0.909 - ETA: 0s - loss: 0.2437 - acc: 0.912 - ETA: 0s - loss: 0.2406 - acc: 0.915 - ETA: 0s - loss: 0.2372 - acc: 0.916 - ETA: 0s - loss: 0.2316 - acc: 0.917 - 3s 1ms/step - loss: 0.2319 - acc: 0.9170 - val_loss: 0.2945 - val_acc: 0.9016\n",
      "Epoch 22/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.1724 - acc: 0.960 - ETA: 2s - loss: 0.1901 - acc: 0.950 - ETA: 2s - loss: 0.2271 - acc: 0.930 - ETA: 2s - loss: 0.2170 - acc: 0.930 - ETA: 1s - loss: 0.1925 - acc: 0.938 - ETA: 1s - loss: 0.1852 - acc: 0.938 - ETA: 1s - loss: 0.1911 - acc: 0.938 - ETA: 1s - loss: 0.1892 - acc: 0.940 - ETA: 1s - loss: 0.1955 - acc: 0.935 - ETA: 1s - loss: 0.2032 - acc: 0.931 - ETA: 1s - loss: 0.1974 - acc: 0.932 - ETA: 0s - loss: 0.2007 - acc: 0.931 - ETA: 0s - loss: 0.1982 - acc: 0.932 - ETA: 0s - loss: 0.1953 - acc: 0.932 - ETA: 0s - loss: 0.2018 - acc: 0.930 - ETA: 0s - loss: 0.2060 - acc: 0.927 - ETA: 0s - loss: 0.2037 - acc: 0.927 - ETA: 0s - loss: 0.2101 - acc: 0.924 - ETA: 0s - loss: 0.2111 - acc: 0.923 - ETA: 0s - loss: 0.2121 - acc: 0.922 - 3s 1ms/step - loss: 0.2119 - acc: 0.9220 - val_loss: 0.2889 - val_acc: 0.9091\n",
      "Epoch 23/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.2317 - acc: 0.910 - ETA: 2s - loss: 0.2321 - acc: 0.915 - ETA: 2s - loss: 0.2292 - acc: 0.920 - ETA: 1s - loss: 0.2064 - acc: 0.927 - ETA: 1s - loss: 0.2003 - acc: 0.924 - ETA: 1s - loss: 0.1916 - acc: 0.926 - ETA: 1s - loss: 0.1990 - acc: 0.922 - ETA: 1s - loss: 0.1930 - acc: 0.926 - ETA: 1s - loss: 0.1871 - acc: 0.930 - ETA: 1s - loss: 0.1931 - acc: 0.931 - ETA: 1s - loss: 0.1912 - acc: 0.931 - ETA: 0s - loss: 0.1968 - acc: 0.929 - ETA: 0s - loss: 0.1969 - acc: 0.928 - ETA: 0s - loss: 0.1968 - acc: 0.927 - ETA: 0s - loss: 0.1940 - acc: 0.929 - ETA: 0s - loss: 0.1958 - acc: 0.927 - ETA: 0s - loss: 0.2041 - acc: 0.925 - ETA: 0s - loss: 0.2089 - acc: 0.923 - ETA: 0s - loss: 0.2059 - acc: 0.924 - ETA: 0s - loss: 0.2048 - acc: 0.925 - 3s 1ms/step - loss: 0.2057 - acc: 0.9245 - val_loss: 0.2940 - val_acc: 0.9098\n",
      "Epoch 24/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.2513 - acc: 0.910 - ETA: 1s - loss: 0.2486 - acc: 0.905 - ETA: 1s - loss: 0.2180 - acc: 0.920 - ETA: 1s - loss: 0.2173 - acc: 0.922 - ETA: 1s - loss: 0.2221 - acc: 0.920 - ETA: 1s - loss: 0.2159 - acc: 0.920 - ETA: 1s - loss: 0.2073 - acc: 0.921 - ETA: 1s - loss: 0.1980 - acc: 0.925 - ETA: 1s - loss: 0.2003 - acc: 0.925 - ETA: 1s - loss: 0.2047 - acc: 0.923 - ETA: 1s - loss: 0.1976 - acc: 0.924 - ETA: 0s - loss: 0.1938 - acc: 0.926 - ETA: 0s - loss: 0.1929 - acc: 0.927 - ETA: 0s - loss: 0.1922 - acc: 0.928 - ETA: 0s - loss: 0.1898 - acc: 0.928 - ETA: 0s - loss: 0.1980 - acc: 0.925 - ETA: 0s - loss: 0.1946 - acc: 0.925 - ETA: 0s - loss: 0.1943 - acc: 0.925 - ETA: 0s - loss: 0.1908 - acc: 0.927 - ETA: 0s - loss: 0.1901 - acc: 0.929 - 3s 1ms/step - loss: 0.1890 - acc: 0.9294 - val_loss: 0.2787 - val_acc: 0.9113\n",
      "Epoch 25/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.1803 - acc: 0.930 - ETA: 2s - loss: 0.1708 - acc: 0.935 - ETA: 2s - loss: 0.1811 - acc: 0.936 - ETA: 2s - loss: 0.1734 - acc: 0.935 - ETA: 1s - loss: 0.1826 - acc: 0.932 - ETA: 1s - loss: 0.1784 - acc: 0.935 - ETA: 1s - loss: 0.1810 - acc: 0.931 - ETA: 1s - loss: 0.1828 - acc: 0.928 - ETA: 1s - loss: 0.1784 - acc: 0.932 - ETA: 1s - loss: 0.1725 - acc: 0.934 - ETA: 1s - loss: 0.1670 - acc: 0.936 - ETA: 1s - loss: 0.1662 - acc: 0.935 - ETA: 0s - loss: 0.1671 - acc: 0.936 - ETA: 0s - loss: 0.1715 - acc: 0.935 - ETA: 0s - loss: 0.1649 - acc: 0.937 - ETA: 0s - loss: 0.1639 - acc: 0.938 - ETA: 0s - loss: 0.1627 - acc: 0.939 - ETA: 0s - loss: 0.1608 - acc: 0.938 - ETA: 0s - loss: 0.1620 - acc: 0.938 - ETA: 0s - loss: 0.1659 - acc: 0.936 - 3s 2ms/step - loss: 0.1664 - acc: 0.9364 - val_loss: 0.3149 - val_acc: 0.9061\n",
      "Epoch 26/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.2856 - acc: 0.910 - ETA: 2s - loss: 0.1773 - acc: 0.945 - ETA: 2s - loss: 0.1673 - acc: 0.946 - ETA: 1s - loss: 0.1767 - acc: 0.940 - ETA: 1s - loss: 0.1801 - acc: 0.942 - ETA: 1s - loss: 0.1670 - acc: 0.945 - ETA: 1s - loss: 0.1673 - acc: 0.944 - ETA: 1s - loss: 0.1582 - acc: 0.948 - ETA: 1s - loss: 0.1618 - acc: 0.946 - ETA: 1s - loss: 0.1583 - acc: 0.947 - ETA: 1s - loss: 0.1601 - acc: 0.945 - ETA: 1s - loss: 0.1643 - acc: 0.944 - ETA: 0s - loss: 0.1698 - acc: 0.941 - ETA: 0s - loss: 0.1657 - acc: 0.942 - ETA: 0s - loss: 0.1702 - acc: 0.942 - ETA: 0s - loss: 0.1659 - acc: 0.943 - ETA: 0s - loss: 0.1758 - acc: 0.938 - ETA: 0s - loss: 0.1717 - acc: 0.940 - ETA: 0s - loss: 0.1689 - acc: 0.940 - ETA: 0s - loss: 0.1689 - acc: 0.940 - 3s 2ms/step - loss: 0.1686 - acc: 0.9404 - val_loss: 0.3038 - val_acc: 0.9113\n",
      "Epoch 27/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.1012 - acc: 0.970 - ETA: 2s - loss: 0.0822 - acc: 0.980 - ETA: 2s - loss: 0.1086 - acc: 0.966 - ETA: 2s - loss: 0.1246 - acc: 0.962 - ETA: 1s - loss: 0.1211 - acc: 0.958 - ETA: 1s - loss: 0.1261 - acc: 0.956 - ETA: 1s - loss: 0.1348 - acc: 0.954 - ETA: 1s - loss: 0.1463 - acc: 0.947 - ETA: 1s - loss: 0.1472 - acc: 0.946 - ETA: 1s - loss: 0.1474 - acc: 0.947 - ETA: 1s - loss: 0.1467 - acc: 0.947 - ETA: 1s - loss: 0.1431 - acc: 0.948 - ETA: 0s - loss: 0.1455 - acc: 0.946 - ETA: 0s - loss: 0.1510 - acc: 0.945 - ETA: 0s - loss: 0.1519 - acc: 0.944 - ETA: 0s - loss: 0.1613 - acc: 0.939 - ETA: 0s - loss: 0.1630 - acc: 0.937 - ETA: 0s - loss: 0.1633 - acc: 0.937 - ETA: 0s - loss: 0.1579 - acc: 0.939 - ETA: 0s - loss: 0.1611 - acc: 0.938 - 3s 2ms/step - loss: 0.1612 - acc: 0.9384 - val_loss: 0.3104 - val_acc: 0.9069\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012/2012 [==============================] - ETA: 2s - loss: 0.1804 - acc: 0.950 - ETA: 2s - loss: 0.1293 - acc: 0.965 - ETA: 2s - loss: 0.1522 - acc: 0.946 - ETA: 1s - loss: 0.1465 - acc: 0.945 - ETA: 1s - loss: 0.1554 - acc: 0.942 - ETA: 1s - loss: 0.1568 - acc: 0.943 - ETA: 1s - loss: 0.1682 - acc: 0.940 - ETA: 1s - loss: 0.1660 - acc: 0.942 - ETA: 1s - loss: 0.1694 - acc: 0.941 - ETA: 1s - loss: 0.1696 - acc: 0.941 - ETA: 1s - loss: 0.1824 - acc: 0.937 - ETA: 0s - loss: 0.1862 - acc: 0.935 - ETA: 0s - loss: 0.1886 - acc: 0.934 - ETA: 0s - loss: 0.1834 - acc: 0.935 - ETA: 0s - loss: 0.1806 - acc: 0.935 - ETA: 0s - loss: 0.1779 - acc: 0.937 - ETA: 0s - loss: 0.1732 - acc: 0.939 - ETA: 0s - loss: 0.1733 - acc: 0.938 - ETA: 0s - loss: 0.1722 - acc: 0.939 - ETA: 0s - loss: 0.1708 - acc: 0.940 - 3s 1ms/step - loss: 0.1700 - acc: 0.9409 - val_loss: 0.2885 - val_acc: 0.9136\n",
      "Epoch 29/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.1114 - acc: 0.950 - ETA: 2s - loss: 0.1635 - acc: 0.925 - ETA: 2s - loss: 0.1885 - acc: 0.920 - ETA: 1s - loss: 0.1639 - acc: 0.932 - ETA: 1s - loss: 0.1596 - acc: 0.932 - ETA: 1s - loss: 0.1639 - acc: 0.931 - ETA: 1s - loss: 0.1527 - acc: 0.935 - ETA: 1s - loss: 0.1522 - acc: 0.936 - ETA: 1s - loss: 0.1485 - acc: 0.937 - ETA: 1s - loss: 0.1454 - acc: 0.943 - ETA: 1s - loss: 0.1541 - acc: 0.940 - ETA: 0s - loss: 0.1632 - acc: 0.936 - ETA: 0s - loss: 0.1586 - acc: 0.939 - ETA: 0s - loss: 0.1621 - acc: 0.937 - ETA: 0s - loss: 0.1625 - acc: 0.937 - ETA: 0s - loss: 0.1635 - acc: 0.936 - ETA: 0s - loss: 0.1642 - acc: 0.937 - ETA: 0s - loss: 0.1605 - acc: 0.940 - ETA: 0s - loss: 0.1635 - acc: 0.938 - ETA: 0s - loss: 0.1610 - acc: 0.939 - 3s 1ms/step - loss: 0.1601 - acc: 0.9399 - val_loss: 0.3040 - val_acc: 0.9091\n",
      "Epoch 30/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.1103 - acc: 0.960 - ETA: 2s - loss: 0.1224 - acc: 0.955 - ETA: 1s - loss: 0.1338 - acc: 0.946 - ETA: 1s - loss: 0.1348 - acc: 0.940 - ETA: 1s - loss: 0.1403 - acc: 0.938 - ETA: 1s - loss: 0.1550 - acc: 0.935 - ETA: 1s - loss: 0.1556 - acc: 0.937 - ETA: 1s - loss: 0.1464 - acc: 0.942 - ETA: 1s - loss: 0.1399 - acc: 0.945 - ETA: 1s - loss: 0.1368 - acc: 0.946 - ETA: 1s - loss: 0.1425 - acc: 0.944 - ETA: 0s - loss: 0.1393 - acc: 0.946 - ETA: 0s - loss: 0.1343 - acc: 0.949 - ETA: 0s - loss: 0.1358 - acc: 0.949 - ETA: 0s - loss: 0.1361 - acc: 0.948 - ETA: 0s - loss: 0.1387 - acc: 0.948 - ETA: 0s - loss: 0.1368 - acc: 0.949 - ETA: 0s - loss: 0.1375 - acc: 0.948 - ETA: 0s - loss: 0.1393 - acc: 0.947 - ETA: 0s - loss: 0.1432 - acc: 0.947 - 3s 1ms/step - loss: 0.1435 - acc: 0.9473 - val_loss: 0.3319 - val_acc: 0.9031\n",
      "Epoch 31/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.2924 - acc: 0.890 - ETA: 2s - loss: 0.1818 - acc: 0.930 - ETA: 1s - loss: 0.1849 - acc: 0.936 - ETA: 1s - loss: 0.1555 - acc: 0.947 - ETA: 1s - loss: 0.1536 - acc: 0.948 - ETA: 1s - loss: 0.1472 - acc: 0.953 - ETA: 1s - loss: 0.1377 - acc: 0.954 - ETA: 1s - loss: 0.1418 - acc: 0.952 - ETA: 1s - loss: 0.1371 - acc: 0.954 - ETA: 1s - loss: 0.1320 - acc: 0.956 - ETA: 1s - loss: 0.1345 - acc: 0.954 - ETA: 0s - loss: 0.1377 - acc: 0.950 - ETA: 0s - loss: 0.1358 - acc: 0.951 - ETA: 0s - loss: 0.1395 - acc: 0.949 - ETA: 0s - loss: 0.1398 - acc: 0.948 - ETA: 0s - loss: 0.1409 - acc: 0.949 - ETA: 0s - loss: 0.1438 - acc: 0.948 - ETA: 0s - loss: 0.1462 - acc: 0.948 - ETA: 0s - loss: 0.1441 - acc: 0.949 - ETA: 0s - loss: 0.1487 - acc: 0.947 - 3s 1ms/step - loss: 0.1490 - acc: 0.9468 - val_loss: 0.3146 - val_acc: 0.9061\n",
      "Epoch 32/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.1061 - acc: 0.970 - ETA: 2s - loss: 0.1302 - acc: 0.970 - ETA: 1s - loss: 0.1234 - acc: 0.966 - ETA: 1s - loss: 0.1185 - acc: 0.962 - ETA: 1s - loss: 0.1396 - acc: 0.950 - ETA: 1s - loss: 0.1405 - acc: 0.951 - ETA: 1s - loss: 0.1424 - acc: 0.948 - ETA: 1s - loss: 0.1291 - acc: 0.953 - ETA: 1s - loss: 0.1210 - acc: 0.956 - ETA: 1s - loss: 0.1171 - acc: 0.959 - ETA: 1s - loss: 0.1150 - acc: 0.960 - ETA: 0s - loss: 0.1159 - acc: 0.960 - ETA: 0s - loss: 0.1154 - acc: 0.960 - ETA: 0s - loss: 0.1190 - acc: 0.957 - ETA: 0s - loss: 0.1171 - acc: 0.958 - ETA: 0s - loss: 0.1160 - acc: 0.958 - ETA: 0s - loss: 0.1209 - acc: 0.955 - ETA: 0s - loss: 0.1268 - acc: 0.953 - ETA: 0s - loss: 0.1259 - acc: 0.954 - ETA: 0s - loss: 0.1267 - acc: 0.954 - 3s 1ms/step - loss: 0.1260 - acc: 0.9548 - val_loss: 0.3070 - val_acc: 0.9098\n",
      "Epoch 33/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.1814 - acc: 0.940 - ETA: 2s - loss: 0.1286 - acc: 0.960 - ETA: 1s - loss: 0.1375 - acc: 0.960 - ETA: 1s - loss: 0.1211 - acc: 0.967 - ETA: 1s - loss: 0.1208 - acc: 0.966 - ETA: 1s - loss: 0.1180 - acc: 0.968 - ETA: 1s - loss: 0.1111 - acc: 0.968 - ETA: 1s - loss: 0.1035 - acc: 0.971 - ETA: 1s - loss: 0.1064 - acc: 0.968 - ETA: 1s - loss: 0.1096 - acc: 0.967 - ETA: 1s - loss: 0.1064 - acc: 0.968 - ETA: 1s - loss: 0.1049 - acc: 0.968 - ETA: 0s - loss: 0.1081 - acc: 0.966 - ETA: 0s - loss: 0.1147 - acc: 0.964 - ETA: 0s - loss: 0.1116 - acc: 0.965 - ETA: 0s - loss: 0.1121 - acc: 0.965 - ETA: 0s - loss: 0.1134 - acc: 0.964 - ETA: 0s - loss: 0.1108 - acc: 0.965 - ETA: 0s - loss: 0.1101 - acc: 0.965 - ETA: 0s - loss: 0.1111 - acc: 0.965 - 3s 1ms/step - loss: 0.1120 - acc: 0.9647 - val_loss: 0.3595 - val_acc: 0.9076\n",
      "Epoch 34/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.1198 - acc: 0.960 - ETA: 2s - loss: 0.1086 - acc: 0.965 - ETA: 1s - loss: 0.1178 - acc: 0.963 - ETA: 1s - loss: 0.1130 - acc: 0.965 - ETA: 1s - loss: 0.1167 - acc: 0.962 - ETA: 1s - loss: 0.1274 - acc: 0.953 - ETA: 1s - loss: 0.1251 - acc: 0.957 - ETA: 1s - loss: 0.1247 - acc: 0.957 - ETA: 1s - loss: 0.1295 - acc: 0.956 - ETA: 1s - loss: 0.1297 - acc: 0.959 - ETA: 1s - loss: 0.1260 - acc: 0.959 - ETA: 1s - loss: 0.1251 - acc: 0.960 - ETA: 0s - loss: 0.1268 - acc: 0.959 - ETA: 0s - loss: 0.1307 - acc: 0.956 - ETA: 0s - loss: 0.1277 - acc: 0.956 - ETA: 0s - loss: 0.1226 - acc: 0.957 - ETA: 0s - loss: 0.1243 - acc: 0.957 - ETA: 0s - loss: 0.1199 - acc: 0.958 - ETA: 0s - loss: 0.1188 - acc: 0.959 - ETA: 0s - loss: 0.1215 - acc: 0.958 - 3s 1ms/step - loss: 0.1208 - acc: 0.9583 - val_loss: 0.3737 - val_acc: 0.9151\n",
      "Epoch 35/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.1581 - acc: 0.950 - ETA: 2s - loss: 0.1324 - acc: 0.955 - ETA: 1s - loss: 0.1265 - acc: 0.953 - ETA: 1s - loss: 0.1264 - acc: 0.952 - ETA: 1s - loss: 0.1220 - acc: 0.958 - ETA: 1s - loss: 0.1264 - acc: 0.958 - ETA: 1s - loss: 0.1121 - acc: 0.964 - ETA: 1s - loss: 0.1107 - acc: 0.963 - ETA: 1s - loss: 0.1098 - acc: 0.962 - ETA: 1s - loss: 0.1120 - acc: 0.959 - ETA: 1s - loss: 0.1079 - acc: 0.960 - ETA: 0s - loss: 0.1037 - acc: 0.962 - ETA: 0s - loss: 0.1086 - acc: 0.960 - ETA: 0s - loss: 0.1064 - acc: 0.960 - ETA: 0s - loss: 0.1040 - acc: 0.960 - ETA: 0s - loss: 0.1033 - acc: 0.961 - ETA: 0s - loss: 0.1042 - acc: 0.960 - ETA: 0s - loss: 0.1022 - acc: 0.960 - ETA: 0s - loss: 0.1151 - acc: 0.958 - ETA: 0s - loss: 0.1131 - acc: 0.959 - 3s 1ms/step - loss: 0.1132 - acc: 0.9592 - val_loss: 0.3313 - val_acc: 0.9098\n",
      "Epoch 36/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.0757 - acc: 0.960 - ETA: 2s - loss: 0.1253 - acc: 0.945 - ETA: 2s - loss: 0.1210 - acc: 0.950 - ETA: 1s - loss: 0.1160 - acc: 0.957 - ETA: 1s - loss: 0.1114 - acc: 0.962 - ETA: 1s - loss: 0.1092 - acc: 0.963 - ETA: 1s - loss: 0.1178 - acc: 0.958 - ETA: 1s - loss: 0.1145 - acc: 0.960 - ETA: 1s - loss: 0.1083 - acc: 0.963 - ETA: 1s - loss: 0.1026 - acc: 0.965 - ETA: 1s - loss: 0.1077 - acc: 0.962 - ETA: 0s - loss: 0.1033 - acc: 0.965 - ETA: 0s - loss: 0.1051 - acc: 0.965 - ETA: 0s - loss: 0.1092 - acc: 0.964 - ETA: 0s - loss: 0.1086 - acc: 0.964 - ETA: 0s - loss: 0.1061 - acc: 0.964 - ETA: 0s - loss: 0.1028 - acc: 0.965 - ETA: 0s - loss: 0.1007 - acc: 0.967 - ETA: 0s - loss: 0.1004 - acc: 0.967 - ETA: 0s - loss: 0.0999 - acc: 0.967 - 3s 1ms/step - loss: 0.0993 - acc: 0.9677 - val_loss: 0.3238 - val_acc: 0.9091\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012/2012 [==============================] - ETA: 2s - loss: 0.1135 - acc: 0.950 - ETA: 1s - loss: 0.1325 - acc: 0.935 - ETA: 1s - loss: 0.1117 - acc: 0.956 - ETA: 1s - loss: 0.1088 - acc: 0.960 - ETA: 1s - loss: 0.0929 - acc: 0.966 - ETA: 1s - loss: 0.1019 - acc: 0.958 - ETA: 1s - loss: 0.1024 - acc: 0.958 - ETA: 1s - loss: 0.1001 - acc: 0.960 - ETA: 1s - loss: 0.1083 - acc: 0.961 - ETA: 1s - loss: 0.1041 - acc: 0.962 - ETA: 0s - loss: 0.0992 - acc: 0.965 - ETA: 0s - loss: 0.0951 - acc: 0.966 - ETA: 0s - loss: 0.0989 - acc: 0.965 - ETA: 0s - loss: 0.0974 - acc: 0.966 - ETA: 0s - loss: 0.0977 - acc: 0.966 - ETA: 0s - loss: 0.0968 - acc: 0.966 - ETA: 0s - loss: 0.0964 - acc: 0.966 - ETA: 0s - loss: 0.0945 - acc: 0.966 - ETA: 0s - loss: 0.0919 - acc: 0.966 - ETA: 0s - loss: 0.0901 - acc: 0.967 - 3s 1ms/step - loss: 0.0896 - acc: 0.9672 - val_loss: 0.3431 - val_acc: 0.9136\n",
      "Epoch 38/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.0988 - acc: 0.970 - ETA: 2s - loss: 0.0906 - acc: 0.975 - ETA: 1s - loss: 0.1118 - acc: 0.966 - ETA: 1s - loss: 0.1106 - acc: 0.965 - ETA: 1s - loss: 0.0967 - acc: 0.970 - ETA: 1s - loss: 0.1049 - acc: 0.966 - ETA: 1s - loss: 0.1009 - acc: 0.962 - ETA: 1s - loss: 0.1069 - acc: 0.960 - ETA: 1s - loss: 0.1036 - acc: 0.961 - ETA: 1s - loss: 0.1003 - acc: 0.963 - ETA: 0s - loss: 0.0967 - acc: 0.964 - ETA: 0s - loss: 0.0952 - acc: 0.965 - ETA: 0s - loss: 0.1033 - acc: 0.963 - ETA: 0s - loss: 0.1028 - acc: 0.962 - ETA: 0s - loss: 0.1031 - acc: 0.962 - ETA: 0s - loss: 0.1028 - acc: 0.963 - ETA: 0s - loss: 0.1004 - acc: 0.963 - ETA: 0s - loss: 0.1006 - acc: 0.962 - ETA: 0s - loss: 0.0986 - acc: 0.962 - ETA: 0s - loss: 0.0965 - acc: 0.964 - 3s 1ms/step - loss: 0.0963 - acc: 0.9642 - val_loss: 0.4256 - val_acc: 0.9031\n",
      "Epoch 39/50\n",
      "2012/2012 [==============================] - ETA: 1s - loss: 0.1427 - acc: 0.920 - ETA: 1s - loss: 0.1226 - acc: 0.950 - ETA: 1s - loss: 0.1093 - acc: 0.956 - ETA: 1s - loss: 0.1024 - acc: 0.960 - ETA: 1s - loss: 0.0981 - acc: 0.964 - ETA: 1s - loss: 0.0868 - acc: 0.970 - ETA: 1s - loss: 0.0803 - acc: 0.972 - ETA: 1s - loss: 0.0912 - acc: 0.970 - ETA: 1s - loss: 0.0934 - acc: 0.967 - ETA: 1s - loss: 0.0915 - acc: 0.967 - ETA: 0s - loss: 0.0854 - acc: 0.970 - ETA: 0s - loss: 0.0912 - acc: 0.968 - ETA: 0s - loss: 0.0931 - acc: 0.966 - ETA: 0s - loss: 0.0962 - acc: 0.967 - ETA: 0s - loss: 0.0988 - acc: 0.964 - ETA: 0s - loss: 0.0976 - acc: 0.965 - ETA: 0s - loss: 0.1043 - acc: 0.964 - ETA: 0s - loss: 0.1101 - acc: 0.962 - ETA: 0s - loss: 0.1109 - acc: 0.963 - ETA: 0s - loss: 0.1099 - acc: 0.964 - 3s 1ms/step - loss: 0.1124 - acc: 0.9632 - val_loss: 0.4057 - val_acc: 0.9024\n",
      "Epoch 40/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.1027 - acc: 0.960 - ETA: 1s - loss: 0.1002 - acc: 0.965 - ETA: 1s - loss: 0.1230 - acc: 0.956 - ETA: 1s - loss: 0.1184 - acc: 0.962 - ETA: 1s - loss: 0.1108 - acc: 0.964 - ETA: 1s - loss: 0.1110 - acc: 0.965 - ETA: 1s - loss: 0.1163 - acc: 0.962 - ETA: 1s - loss: 0.1060 - acc: 0.965 - ETA: 1s - loss: 0.1181 - acc: 0.963 - ETA: 1s - loss: 0.1167 - acc: 0.963 - ETA: 0s - loss: 0.1105 - acc: 0.964 - ETA: 0s - loss: 0.1056 - acc: 0.965 - ETA: 0s - loss: 0.1040 - acc: 0.966 - ETA: 0s - loss: 0.1062 - acc: 0.965 - ETA: 0s - loss: 0.1115 - acc: 0.963 - ETA: 0s - loss: 0.1136 - acc: 0.963 - ETA: 0s - loss: 0.1077 - acc: 0.965 - ETA: 0s - loss: 0.1064 - acc: 0.966 - ETA: 0s - loss: 0.1056 - acc: 0.965 - ETA: 0s - loss: 0.1071 - acc: 0.965 - 3s 1ms/step - loss: 0.1076 - acc: 0.9652 - val_loss: 0.3646 - val_acc: 0.9151\n",
      "Epoch 41/50\n",
      "2012/2012 [==============================] - ETA: 1s - loss: 0.1353 - acc: 0.980 - ETA: 1s - loss: 0.0804 - acc: 0.990 - ETA: 1s - loss: 0.0937 - acc: 0.976 - ETA: 1s - loss: 0.0817 - acc: 0.980 - ETA: 1s - loss: 0.0860 - acc: 0.974 - ETA: 1s - loss: 0.0913 - acc: 0.973 - ETA: 1s - loss: 0.0898 - acc: 0.972 - ETA: 1s - loss: 0.0887 - acc: 0.975 - ETA: 1s - loss: 0.0922 - acc: 0.971 - ETA: 1s - loss: 0.0911 - acc: 0.972 - ETA: 0s - loss: 0.0876 - acc: 0.973 - ETA: 0s - loss: 0.0833 - acc: 0.975 - ETA: 0s - loss: 0.0859 - acc: 0.973 - ETA: 0s - loss: 0.0873 - acc: 0.971 - ETA: 0s - loss: 0.0842 - acc: 0.972 - ETA: 0s - loss: 0.0824 - acc: 0.973 - ETA: 0s - loss: 0.0825 - acc: 0.972 - ETA: 0s - loss: 0.0806 - acc: 0.973 - ETA: 0s - loss: 0.0820 - acc: 0.973 - ETA: 0s - loss: 0.0806 - acc: 0.974 - 3s 1ms/step - loss: 0.0813 - acc: 0.9737 - val_loss: 0.3499 - val_acc: 0.9113\n",
      "Epoch 42/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.0869 - acc: 0.990 - ETA: 1s - loss: 0.0712 - acc: 0.985 - ETA: 1s - loss: 0.0614 - acc: 0.986 - ETA: 1s - loss: 0.0609 - acc: 0.987 - ETA: 1s - loss: 0.0621 - acc: 0.984 - ETA: 1s - loss: 0.0579 - acc: 0.986 - ETA: 1s - loss: 0.0559 - acc: 0.985 - ETA: 1s - loss: 0.0596 - acc: 0.983 - ETA: 1s - loss: 0.0654 - acc: 0.978 - ETA: 1s - loss: 0.0666 - acc: 0.980 - ETA: 1s - loss: 0.0648 - acc: 0.980 - ETA: 0s - loss: 0.0672 - acc: 0.978 - ETA: 0s - loss: 0.0715 - acc: 0.977 - ETA: 0s - loss: 0.0726 - acc: 0.977 - ETA: 0s - loss: 0.0709 - acc: 0.978 - ETA: 0s - loss: 0.0728 - acc: 0.977 - ETA: 0s - loss: 0.0725 - acc: 0.977 - ETA: 0s - loss: 0.0710 - acc: 0.978 - ETA: 0s - loss: 0.0705 - acc: 0.977 - ETA: 0s - loss: 0.0733 - acc: 0.977 - 3s 1ms/step - loss: 0.0732 - acc: 0.9771 - val_loss: 0.3715 - val_acc: 0.9180\n",
      "Epoch 43/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.0318 - acc: 0.990 - ETA: 1s - loss: 0.0411 - acc: 0.985 - ETA: 1s - loss: 0.0532 - acc: 0.983 - ETA: 1s - loss: 0.0508 - acc: 0.985 - ETA: 1s - loss: 0.0526 - acc: 0.984 - ETA: 1s - loss: 0.0560 - acc: 0.981 - ETA: 1s - loss: 0.0534 - acc: 0.982 - ETA: 1s - loss: 0.0577 - acc: 0.980 - ETA: 1s - loss: 0.0595 - acc: 0.977 - ETA: 1s - loss: 0.0628 - acc: 0.976 - ETA: 0s - loss: 0.0612 - acc: 0.977 - ETA: 0s - loss: 0.0598 - acc: 0.976 - ETA: 0s - loss: 0.0573 - acc: 0.978 - ETA: 0s - loss: 0.0597 - acc: 0.976 - ETA: 0s - loss: 0.0577 - acc: 0.977 - ETA: 0s - loss: 0.0585 - acc: 0.977 - ETA: 0s - loss: 0.0570 - acc: 0.978 - ETA: 0s - loss: 0.0599 - acc: 0.978 - ETA: 0s - loss: 0.0641 - acc: 0.977 - ETA: 0s - loss: 0.0632 - acc: 0.978 - 3s 1ms/step - loss: 0.0630 - acc: 0.9786 - val_loss: 0.4046 - val_acc: 0.9121\n",
      "Epoch 44/50\n",
      "2012/2012 [==============================] - ETA: 1s - loss: 0.0603 - acc: 0.980 - ETA: 1s - loss: 0.1206 - acc: 0.975 - ETA: 1s - loss: 0.1141 - acc: 0.976 - ETA: 1s - loss: 0.0888 - acc: 0.982 - ETA: 1s - loss: 0.0822 - acc: 0.982 - ETA: 1s - loss: 0.0795 - acc: 0.981 - ETA: 1s - loss: 0.0727 - acc: 0.982 - ETA: 1s - loss: 0.0713 - acc: 0.982 - ETA: 1s - loss: 0.0656 - acc: 0.984 - ETA: 1s - loss: 0.0632 - acc: 0.985 - ETA: 0s - loss: 0.0667 - acc: 0.982 - ETA: 0s - loss: 0.0643 - acc: 0.982 - ETA: 0s - loss: 0.0660 - acc: 0.982 - ETA: 0s - loss: 0.0624 - acc: 0.983 - ETA: 0s - loss: 0.0620 - acc: 0.983 - ETA: 0s - loss: 0.0622 - acc: 0.984 - ETA: 0s - loss: 0.0607 - acc: 0.984 - ETA: 0s - loss: 0.0613 - acc: 0.983 - ETA: 0s - loss: 0.0628 - acc: 0.983 - ETA: 0s - loss: 0.0624 - acc: 0.984 - 3s 1ms/step - loss: 0.0631 - acc: 0.9836 - val_loss: 0.3915 - val_acc: 0.9151\n",
      "Epoch 45/50\n",
      "2012/2012 [==============================] - ETA: 1s - loss: 0.0473 - acc: 0.980 - ETA: 1s - loss: 0.0538 - acc: 0.980 - ETA: 1s - loss: 0.0681 - acc: 0.973 - ETA: 1s - loss: 0.0629 - acc: 0.977 - ETA: 1s - loss: 0.0695 - acc: 0.978 - ETA: 1s - loss: 0.0684 - acc: 0.976 - ETA: 1s - loss: 0.0642 - acc: 0.978 - ETA: 1s - loss: 0.0633 - acc: 0.978 - ETA: 1s - loss: 0.0692 - acc: 0.975 - ETA: 1s - loss: 0.0681 - acc: 0.975 - ETA: 0s - loss: 0.0629 - acc: 0.977 - ETA: 0s - loss: 0.0640 - acc: 0.977 - ETA: 0s - loss: 0.0645 - acc: 0.976 - ETA: 0s - loss: 0.0636 - acc: 0.976 - ETA: 0s - loss: 0.0643 - acc: 0.976 - ETA: 0s - loss: 0.0630 - acc: 0.976 - ETA: 0s - loss: 0.0601 - acc: 0.977 - ETA: 0s - loss: 0.0610 - acc: 0.978 - ETA: 0s - loss: 0.0619 - acc: 0.977 - ETA: 0s - loss: 0.0640 - acc: 0.977 - 3s 1ms/step - loss: 0.0637 - acc: 0.9776 - val_loss: 0.4059 - val_acc: 0.9143\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012/2012 [==============================] - ETA: 2s - loss: 0.0774 - acc: 0.960 - ETA: 1s - loss: 0.0520 - acc: 0.970 - ETA: 1s - loss: 0.0450 - acc: 0.973 - ETA: 1s - loss: 0.0444 - acc: 0.977 - ETA: 1s - loss: 0.0507 - acc: 0.974 - ETA: 1s - loss: 0.0547 - acc: 0.975 - ETA: 1s - loss: 0.0618 - acc: 0.972 - ETA: 1s - loss: 0.0594 - acc: 0.975 - ETA: 1s - loss: 0.0622 - acc: 0.975 - ETA: 1s - loss: 0.0696 - acc: 0.975 - ETA: 0s - loss: 0.0741 - acc: 0.973 - ETA: 0s - loss: 0.0749 - acc: 0.973 - ETA: 0s - loss: 0.0718 - acc: 0.974 - ETA: 0s - loss: 0.0708 - acc: 0.975 - ETA: 0s - loss: 0.0692 - acc: 0.975 - ETA: 0s - loss: 0.0681 - acc: 0.975 - ETA: 0s - loss: 0.0683 - acc: 0.975 - ETA: 0s - loss: 0.0662 - acc: 0.975 - ETA: 0s - loss: 0.0655 - acc: 0.975 - ETA: 0s - loss: 0.0634 - acc: 0.976 - 3s 1ms/step - loss: 0.0635 - acc: 0.9761 - val_loss: 0.3669 - val_acc: 0.9173\n",
      "Epoch 47/50\n",
      "2012/2012 [==============================] - ETA: 1s - loss: 0.0712 - acc: 0.970 - ETA: 1s - loss: 0.0491 - acc: 0.985 - ETA: 1s - loss: 0.0465 - acc: 0.986 - ETA: 1s - loss: 0.0598 - acc: 0.977 - ETA: 1s - loss: 0.0654 - acc: 0.978 - ETA: 1s - loss: 0.0595 - acc: 0.980 - ETA: 1s - loss: 0.0678 - acc: 0.978 - ETA: 1s - loss: 0.0657 - acc: 0.980 - ETA: 1s - loss: 0.0659 - acc: 0.980 - ETA: 1s - loss: 0.0643 - acc: 0.980 - ETA: 0s - loss: 0.0609 - acc: 0.981 - ETA: 0s - loss: 0.0590 - acc: 0.982 - ETA: 0s - loss: 0.0592 - acc: 0.982 - ETA: 0s - loss: 0.0600 - acc: 0.982 - ETA: 0s - loss: 0.0582 - acc: 0.982 - ETA: 0s - loss: 0.0583 - acc: 0.983 - ETA: 0s - loss: 0.0560 - acc: 0.984 - ETA: 0s - loss: 0.0552 - acc: 0.984 - ETA: 0s - loss: 0.0564 - acc: 0.983 - ETA: 0s - loss: 0.0561 - acc: 0.983 - 3s 1ms/step - loss: 0.0558 - acc: 0.9831 - val_loss: 0.4031 - val_acc: 0.9121\n",
      "Epoch 48/50\n",
      "2012/2012 [==============================] - ETA: 1s - loss: 0.0336 - acc: 0.990 - ETA: 1s - loss: 0.0371 - acc: 0.990 - ETA: 1s - loss: 0.0322 - acc: 0.990 - ETA: 1s - loss: 0.0413 - acc: 0.987 - ETA: 1s - loss: 0.0416 - acc: 0.988 - ETA: 1s - loss: 0.0482 - acc: 0.983 - ETA: 1s - loss: 0.0443 - acc: 0.985 - ETA: 1s - loss: 0.0406 - acc: 0.987 - ETA: 1s - loss: 0.0387 - acc: 0.988 - ETA: 1s - loss: 0.0407 - acc: 0.989 - ETA: 0s - loss: 0.0410 - acc: 0.989 - ETA: 0s - loss: 0.0411 - acc: 0.989 - ETA: 0s - loss: 0.0396 - acc: 0.989 - ETA: 0s - loss: 0.0417 - acc: 0.987 - ETA: 0s - loss: 0.0424 - acc: 0.987 - ETA: 0s - loss: 0.0426 - acc: 0.986 - ETA: 0s - loss: 0.0429 - acc: 0.986 - ETA: 0s - loss: 0.0420 - acc: 0.986 - ETA: 0s - loss: 0.0432 - acc: 0.985 - ETA: 0s - loss: 0.0436 - acc: 0.985 - 3s 1ms/step - loss: 0.0433 - acc: 0.9851 - val_loss: 0.3851 - val_acc: 0.9180\n",
      "Epoch 49/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.0336 - acc: 0.990 - ETA: 2s - loss: 0.0432 - acc: 0.985 - ETA: 1s - loss: 0.0507 - acc: 0.983 - ETA: 1s - loss: 0.0513 - acc: 0.985 - ETA: 1s - loss: 0.0566 - acc: 0.984 - ETA: 1s - loss: 0.0526 - acc: 0.985 - ETA: 1s - loss: 0.0555 - acc: 0.984 - ETA: 1s - loss: 0.0566 - acc: 0.982 - ETA: 1s - loss: 0.0510 - acc: 0.984 - ETA: 1s - loss: 0.0552 - acc: 0.984 - ETA: 1s - loss: 0.0513 - acc: 0.985 - ETA: 0s - loss: 0.0500 - acc: 0.985 - ETA: 0s - loss: 0.0492 - acc: 0.986 - ETA: 0s - loss: 0.0484 - acc: 0.986 - ETA: 0s - loss: 0.0518 - acc: 0.986 - ETA: 0s - loss: 0.0520 - acc: 0.986 - ETA: 0s - loss: 0.0523 - acc: 0.985 - ETA: 0s - loss: 0.0566 - acc: 0.983 - ETA: 0s - loss: 0.0615 - acc: 0.982 - ETA: 0s - loss: 0.0625 - acc: 0.982 - 3s 1ms/step - loss: 0.0628 - acc: 0.9821 - val_loss: 0.3841 - val_acc: 0.9165\n",
      "Epoch 50/50\n",
      "2012/2012 [==============================] - ETA: 2s - loss: 0.0537 - acc: 0.980 - ETA: 1s - loss: 0.0418 - acc: 0.985 - ETA: 1s - loss: 0.0437 - acc: 0.983 - ETA: 1s - loss: 0.0446 - acc: 0.985 - ETA: 1s - loss: 0.0390 - acc: 0.986 - ETA: 1s - loss: 0.0445 - acc: 0.985 - ETA: 1s - loss: 0.0465 - acc: 0.984 - ETA: 1s - loss: 0.0487 - acc: 0.983 - ETA: 1s - loss: 0.0485 - acc: 0.984 - ETA: 1s - loss: 0.0471 - acc: 0.986 - ETA: 1s - loss: 0.0464 - acc: 0.985 - ETA: 1s - loss: 0.0454 - acc: 0.985 - ETA: 0s - loss: 0.0448 - acc: 0.986 - ETA: 0s - loss: 0.0449 - acc: 0.985 - ETA: 0s - loss: 0.0437 - acc: 0.986 - ETA: 0s - loss: 0.0418 - acc: 0.986 - ETA: 0s - loss: 0.0400 - acc: 0.987 - ETA: 0s - loss: 0.0413 - acc: 0.987 - ETA: 0s - loss: 0.0405 - acc: 0.987 - ETA: 0s - loss: 0.0420 - acc: 0.987 - 3s 1ms/step - loss: 0.0421 - acc: 0.9871 - val_loss: 0.4051 - val_acc: 0.9136\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "history = model.fit(X_train, y_train_hot, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(X_test, y_test_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export current model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported to folder ./models/20181212_21-50-26\n"
     ]
    }
   ],
   "source": [
    "#export_model(model)\n",
    "\n",
    "y_predicted = model.predict_classes(X_test, batch_size=batch_size)\n",
    "y_true_val = np.argmax(y_test_hot,axis=1)\n",
    "\n",
    "class_rep = classification_report(y_true_val,y_predicted,digits=5)\n",
    "\n",
    "settings = {\n",
    "    \"feature_dim_1\": feature_dim_1,\n",
    "    \"feature_dim_2\": feature_dim_2,\n",
    "    \"channel\": channel,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"train_accuracy\": str(history.history.get('acc')[-1]),\n",
    "    \"test_accuracy\": str(history.history.get('val_acc')[-1]),\n",
    "    \"train_loss\": str(history.history.get('loss')[-1]),\n",
    "    \"test_loss\": str(history.history.get('val_loss')[-1]),\n",
    "    \"classification_report\": class_rep,\n",
    "}\n",
    "\n",
    "print(export_model(model, settings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_model(PATH)\n",
    "imported_model = import_model(\"./models/xxx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on a new file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(predict('./test_audio/12.wav', model=model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on a folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.wav was predicted as: 11\n",
      "10.wav was predicted as: 10\n",
      "11.wav was predicted as: 11\n",
      "12.wav was predicted as: 10\n",
      "2.wav was predicted as: 2\n",
      "3.wav was predicted as: 3\n",
      "4.wav was predicted as: 5\n",
      "5.wav was predicted as: 5\n",
      "6.wav was predicted as: 6\n",
      "7.wav was predicted as: 7\n",
      "8.wav was predicted as: 8\n",
      "9.wav was predicted as: 9\n"
     ]
    }
   ],
   "source": [
    "FOLDER_PATH = './test_audio/'\n",
    "\n",
    "for filename in os.listdir(FOLDER_PATH):\n",
    "    pred = predict(FOLDER_PATH + filename, model=model)\n",
    "    print(filename + \" was predicted as: \" + pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy / Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Accuracy plot\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#Loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full report with confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9135618479880775\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.91515   0.89881   0.90691       168\n",
      "           1    0.89655   0.88136   0.88889       118\n",
      "           2    0.94318   0.93258   0.93785        89\n",
      "           3    0.89000   0.91753   0.90355        97\n",
      "           4    0.93056   0.90541   0.91781       148\n",
      "           5    0.94615   0.88489   0.91450       139\n",
      "           6    0.93827   0.84444   0.88889        90\n",
      "           7    0.82301   0.94898   0.88152        98\n",
      "           8    0.87156   0.95960   0.91346        99\n",
      "           9    0.88889   0.92632   0.90722        95\n",
      "          10    1.00000   0.95960   0.97938        99\n",
      "          11    0.93137   0.93137   0.93137       102\n",
      "\n",
      "   micro avg    0.91356   0.91356   0.91356      1342\n",
      "   macro avg    0.91456   0.91591   0.91428      1342\n",
      "weighted avg    0.91571   0.91356   0.91377      1342\n",
      "\n",
      "[[151   3   1   0   5   3   0   4   1   0   0   0]\n",
      " [  0 104   0   1   0   0   0   1   8   3   0   1]\n",
      " [  1   1  83   2   0   0   0   0   0   2   0   0]\n",
      " [  0   1   1  89   3   0   0   0   2   0   0   1]\n",
      " [  5   3   0   2 134   4   0   0   0   0   0   0]\n",
      " [  6   0   1   0   2 123   3   2   1   0   0   1]\n",
      " [  1   0   0   2   0   0  76   8   1   1   0   1]\n",
      " [  0   0   0   1   0   0   0  93   1   2   0   1]\n",
      " [  0   0   1   0   0   0   1   1  95   1   0   0]\n",
      " [  0   2   0   2   0   0   0   2   0  88   0   1]\n",
      " [  0   2   0   0   0   0   0   1   0   0  95   1]\n",
      " [  1   0   1   1   0   0   1   1   0   2   0  95]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#should import labels directly from folder:\n",
    "labels, _, _= get_labels(\"./audio\")\n",
    "labArray = []\n",
    "for label in labels:\n",
    "    labArray.append(label)\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit_transform(labArray)\n",
    "\n",
    "full_multiclass_report(model, X_test, y_test_hot, classes=le.inverse_transform(np.arange(12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for all the metrics and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title='Normalized confusion matrix'\n",
    "    else:\n",
    "        title='Confusion matrix'\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "def full_multiclass_report(model,\n",
    "                           x,\n",
    "                           y_true,\n",
    "                           classes,\n",
    "                           batch_size=32,\n",
    "                           binary=False):\n",
    "\n",
    "    # 1. Transform one-hot encoded y_true into their class number\n",
    "    if not binary:\n",
    "        y_true = np.argmax(y_true,axis=1)\n",
    "    \n",
    "    # 2. Predict classes and stores in y_pred\n",
    "    y_pred = model.predict_classes(x, batch_size=batch_size)\n",
    "    \n",
    "    # 3. Print accuracy score\n",
    "    print(\"Accuracy : \"+ str(accuracy_score(y_true,y_pred)))\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    # 4. Print classification report\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_true,y_pred,digits=5))    \n",
    "    \n",
    "    # 5. Plot confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_true,y_pred)\n",
    "    print(cnf_matrix)\n",
    "    plot_confusion_matrix(cnf_matrix,classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Accuracy\n",
    "    plt.figure(2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
